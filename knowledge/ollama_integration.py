#!/usr/bin/env python3
"""
๐ฆ ุชูุงูู Ollama - ูุธุงู ุจุตูุฑุฉ ุงูุซูุฑู
๐ ุฑุจุท ูุคูุช ูุน ููุงุฐุฌ Ollama ุงูููุชูุญุฉ ุงููุตุฏุฑ
๐งฌ ุงุณุชุฎุฑุงุฌ ุงููุนุฑูุฉ ูุชุทุจูู ุงููุธุฑูุงุช ุงูุซูุงุซ

ุงููุฏู: ุจูุงุก ููุชุจุฉ ูุนุฑููุฉ ุฐุงุชูุฉ ูู ุงูููุงุฐุฌ ุงูููุชูุญุฉ
ุงูุงุณุชุฑุงุชูุฌูุฉ: ุงุณุชุฎุฑุงุฌ โ ุชุญููู โ ุญูุธ โ ุงุณุชููุงููุฉ

ุงููุทูุฑ: ุจุงุณู ูุญูู ุนุจุฏุงููู
"""

import requests
import json
import subprocess
import time
from typing import Dict, List, Any, Optional
from knowledge_harvester import KnowledgeHarvester
import os

class OllamaIntegration:
    """
    ๐ฆ ุชูุงูู Ollama ูุน ูุธุงู ุจุตูุฑุฉ
    ูุฑุจุท ูุคูุชุงู ูุน ุงูููุงุฐุฌ ุงูููุชูุญุฉ ูุงุณุชุฎุฑุงุฌ ุงููุนุฑูุฉ
    """
    
    def __init__(self, ollama_url: str = "http://localhost:11434"):
        """ุชููุฆุฉ ุชูุงูู Ollama"""
        self.ollama_url = ollama_url
        self.harvester = KnowledgeHarvester()
        self.available_models = []
        
        print("๐ฆโก ุชู ุฅูุดุงุก ุชูุงูู Ollama")
        print("   ๐ ูุญุงููุฉ ุงูุงุชุตุงู ูุน Ollama...")
        
        if self.check_ollama_status():
            self.load_available_models()
        else:
            print("   โ๏ธ Ollama ุบูุฑ ูุชุงุญ - ุณูุชู ุชุดุบูู ุงููุธุงู ุจุฏููู")
    
    def check_ollama_status(self) -> bool:
        """ูุญุต ุญุงูุฉ Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                print("   โ Ollama ูุชุตู ููุนูู")
                return True
            else:
                print(f"   โ Ollama ูุฑุฏ ุจุญุงูุฉ: {response.status_code}")
                return False
        except requests.exceptions.ConnectionError:
            print("   โ Ollama ุบูุฑ ูุชุตู")
            return False
        except Exception as e:
            print(f"   โ ุฎุทุฃ ูู ุงูุงุชุตุงู: {e}")
            return False
    
    def install_ollama(self) -> bool:
        """ุชุซุจูุช Ollama ุฅุฐุง ูู ููู ููุฌูุฏุงู"""
        try:
            print("๐ฆ ูุญุงููุฉ ุชุซุจูุช Ollama...")
            
            # ุชุญููู ูุชุซุจูุช Ollama
            install_command = "curl -fsSL https://ollama.ai/install.sh | sh"
            result = subprocess.run(install_command, shell=True, capture_output=True, text=True)
            
            if result.returncode == 0:
                print("โ ุชู ุชุซุจูุช Ollama ุจูุฌุงุญ")
                
                # ุจุฏุก ุฎุฏูุฉ Ollama
                print("๐ ุจุฏุก ุฎุฏูุฉ Ollama...")
                subprocess.Popen(["ollama", "serve"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                time.sleep(5)  # ุงูุชุธุงุฑ ุจุฏุก ุงูุฎุฏูุฉ
                
                return self.check_ollama_status()
            else:
                print(f"โ ูุดู ูู ุชุซุจูุช Ollama: {result.stderr}")
                return False
                
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู ุชุซุจูุช Ollama: {e}")
            return False
    
    def load_available_models(self):
        """ุชุญููู ูุงุฆูุฉ ุงูููุงุฐุฌ ุงููุชุงุญุฉ"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                self.available_models = [model['name'] for model in data.get('models', [])]
                print(f"   ๐ ุงูููุงุฐุฌ ุงููุชุงุญุฉ: {self.available_models}")
            else:
                print("   โ ูุดู ูู ุชุญููู ูุงุฆูุฉ ุงูููุงุฐุฌ")
        except Exception as e:
            print(f"   โ ุฎุทุฃ ูู ุชุญููู ุงูููุงุฐุฌ: {e}")
    
    def pull_model(self, model_name: str) -> bool:
        """ุชุญููู ูููุฐุฌ ุฌุฏูุฏ"""
        try:
            print(f"๐ฅ ุชุญููู ุงููููุฐุฌ: {model_name}")
            
            payload = {"name": model_name}
            response = requests.post(
                f"{self.ollama_url}/api/pull",
                json=payload,
                stream=True,
                timeout=300
            )
            
            if response.status_code == 200:
                print(f"โ ุชู ุชุญููู ุงููููุฐุฌ: {model_name}")
                self.load_available_models()  # ุชุญุฏูุซ ุงููุงุฆูุฉ
                return True
            else:
                print(f"โ ูุดู ูู ุชุญููู ุงููููุฐุฌ: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู ุชุญููู ุงููููุฐุฌ: {e}")
            return False
    
    def generate_response(self, prompt: str, model: str = "llama3.2:1b") -> Optional[str]:
        """ุชูููุฏ ุงุณุชุฌุงุจุฉ ูู ุงููููุฐุฌ"""
        if not self.check_ollama_status():
            return None
        
        if model not in self.available_models:
            print(f"โ๏ธ ุงููููุฐุฌ {model} ุบูุฑ ูุชุงุญุ ูุญุงููุฉ ุชุญูููู...")
            if not self.pull_model(model):
                return None
        
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9
                }
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                content = result.get('response', '')
                
                # ุญูุธ ุงูุงุณุชุฌุงุจุฉ ูู ููุชุจุฉ ุงููุนุฑูุฉ
                if content:
                    knowledge_item = self.harvester._create_knowledge_item(
                        content=content,
                        source=f"ollama_{model}",
                        category="llm_response",
                        language="ar"
                    )
                    self.harvester._save_knowledge_item(knowledge_item)
                    print(f"โ ุชู ุญูุธ ุงูุงุณุชุฌุงุจุฉ ูู ููุชุจุฉ ุงููุนุฑูุฉ")
                
                return content
            else:
                print(f"โ ุฎุทุฃ ูู ุชูููุฏ ุงูุงุณุชุฌุงุจุฉ: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"โ ุฎุทุฃ ูู ุงูุชูููุฏ: {e}")
            return None
    
    def extract_knowledge_batch(self, topics: List[str], model: str = "llama3.2:1b") -> int:
        """ุงุณุชุฎุฑุงุฌ ูุนุฑูุฉ ูุฌูุนุฉ ุญูู ููุงุถูุน ูุชุนุฏุฏุฉ"""
        extracted_count = 0
        
        for topic in topics:
            print(f"\n๐ ุงุณุชุฎุฑุงุฌ ูุนุฑูุฉ ุญูู: {topic}")
            
            # ุฅูุดุงุก ุงุณุชุนูุงู ููููุถูุน
            prompt = f"ุงุดุฑุญ ูู ุจุงูุชูุตูู ุนู {topic} ุจุงููุบุฉ ุงูุนุฑุจูุฉ. ูุฏู ูุนูููุงุช ุดุงููุฉ ููููุฏุฉ."
            
            response = self.generate_response(prompt, model)
            if response:
                print(f"   โ ุชู ุงุณุชุฎุฑุงุฌ {len(response)} ุญุฑู")
                extracted_count += 1
            else:
                print(f"   โ ูุดู ูู ุงุณุชุฎุฑุงุฌ ูุนุฑูุฉ ุญูู {topic}")
            
            time.sleep(2)  # ุชุฌูุจ ุงูุถุบุท ุนูู ุงููููุฐุฌ
        
        return extracted_count
    
    def create_knowledge_vectors(self, texts: List[str]) -> List[Dict[str, Any]]:
        """ุฅูุดุงุก ูุชุฌูุงุช ูููุตูุต ุจุงุณุชุฎุฏุงู ุงููุธุฑูุงุช ุงูุซูุฑูุฉ"""
        vectors = []
        
        for text in texts:
            # ุญุณุงุจ ูุชุฌู ุซูุฑู ูููุต
            vector = {
                "text": text[:100] + "..." if len(text) > 100 else text,
                "length": len(text),
                "words": len(text.split()),
                "sentences": len([s for s in text.split('.') if s.strip()]),
                "zero_duality": self.harvester._calculate_zero_duality(text),
                "perpendicularity": self.harvester._calculate_perpendicularity(text),
                "filaments": self.harvester._calculate_filament_connections(text)
            }
            vectors.append(vector)
        
        return vectors
    
    def get_integration_stats(self) -> Dict[str, Any]:
        """ุฅุญุตุงุฆูุงุช ุงูุชูุงูู"""
        knowledge_stats = self.harvester.get_knowledge_stats()
        
        return {
            "ollama_status": self.check_ollama_status(),
            "available_models": self.available_models,
            "knowledge_base": knowledge_stats,
            "integration_active": len(self.available_models) > 0
        }

def setup_recommended_models():
    """ุฅุนุฏุงุฏ ุงูููุงุฐุฌ ุงูููุตู ุจูุง"""
    
    print("๐ฆ ุฅุนุฏุงุฏ ุงูููุงุฐุฌ ุงูููุตู ุจูุง ููุธุงู ุจุตูุฑุฉ")
    print("=" * 60)
    
    integration = OllamaIntegration()
    
    # ุงูููุงุฐุฌ ุงูููุตู ุจูุง (ุตุบูุฑุฉ ูููุงุณุจุฉ)
    recommended_models = [
        "llama2:7b",      # ูููุฐุฌ ุนุงู ุฌูุฏ
        "mistral:7b",     # ูููุฐุฌ ุณุฑูุน ููุนุงู
        "codellama:7b"    # ููุจุฑูุฌุฉ ูุงูุชูููุฉ
    ]
    
    if integration.check_ollama_status():
        print("\n๐ฅ ุชุญููู ุงูููุงุฐุฌ ุงูููุตู ุจูุง:")
        for model in recommended_models:
            print(f"\n๐ ูุญุงููุฉ ุชุญููู: {model}")
            success = integration.pull_model(model)
            if success:
                print(f"โ ุชู ุชุญููู {model} ุจูุฌุงุญ")
            else:
                print(f"โ ูุดู ูู ุชุญููู {model}")
    else:
        print("\nโ๏ธ Ollama ุบูุฑ ูุชุงุญ - ุชุฎุทู ุชุญููู ุงูููุงุฐุฌ")
    
    return integration

def demo_knowledge_extraction():
    """ุนุฑุถ ุชูุถูุญู ูุงุณุชุฎุฑุงุฌ ุงููุนุฑูุฉ"""
    
    print("\n๐พ ุนุฑุถ ุชูุถูุญู ูุงุณุชุฎุฑุงุฌ ุงููุนุฑูุฉ")
    print("=" * 60)
    
    integration = OllamaIntegration()
    
    # ููุงุถูุน ููุงุณุชุฎุฑุงุฌ
    topics = [
        "ุงูุฐูุงุก ุงูุงุตุทูุงุนู",
        "ูุนุงูุฌุฉ ุงููุบุฉ ุงูุทุจูุนูุฉ",
        "ุงูุชุนูู ุงูุขูู",
        "ุงูุฑูุงุถูุงุช ุงูุชุทุจูููุฉ"
    ]
    
    if integration.check_ollama_status() and integration.available_models:
        print(f"\n๐ ุงุณุชุฎุฑุงุฌ ูุนุฑูุฉ ุญูู {len(topics)} ููุงุถูุน:")
        model = integration.available_models[0]  # ุงุณุชุฎุฏุงู ุฃูู ูููุฐุฌ ูุชุงุญ
        
        extracted = integration.extract_knowledge_batch(topics, model)
        print(f"\nโ ุชู ุงุณุชุฎุฑุงุฌ ูุนุฑูุฉ ุญูู {extracted} ููุถูุน")
    else:
        print("\nโ๏ธ ูุง ุชูุฌุฏ ููุงุฐุฌ ูุชุงุญุฉ - ุงุณุชุฎุฏุงู ุงููุนุฑูุฉ ุงููุญููุฉ ููุท")
    
    # ุนุฑุถ ุงูุฅุญุตุงุฆูุงุช
    print("\n๐ ุฅุญุตุงุฆูุงุช ุงูุชูุงูู:")
    stats = integration.get_integration_stats()
    print(f"   ๐ฆ ุญุงูุฉ Ollama: {'ูุชุตู' if stats['ollama_status'] else 'ุบูุฑ ูุชุตู'}")
    print(f"   ๐ ุงูููุงุฐุฌ ุงููุชุงุญุฉ: {len(stats['available_models'])}")
    print(f"   ๐ ูุงุนุฏุฉ ุงููุนุฑูุฉ: {stats['knowledge_base'].get('total_knowledge', 0)} ุนูุตุฑ")
    
    return integration

if __name__ == "__main__":
    # ุชุดุบูู ุงูุนุฑุถ ุงูุชูุถูุญู
    integration = demo_knowledge_extraction()
    
    print("\n๐ ุงูุชูู ุงูุนุฑุถ ุงูุชูุถูุญู!")
    print("๐งฌ ูุธุงู ุงูุชูุงูู ูุน Ollama ุฌุงูุฒ ููุงุณุชุฎุฏุงู!")

#!/usr/bin/env python3
# feedback_mediator_system.py - Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ· Ù„Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©

import sys
import os
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import uuid
from datetime import datetime

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

@dataclass
class FeedbackCycle:
    """Ø¯ÙˆØ±Ø© ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©."""
    cycle_id: str
    original_data: List[Tuple[float, float]]
    inferred_equations: Dict[str, Any]
    generated_data: List[Tuple[float, float]]
    quality_metrics: Any
    feedback: Dict[str, Any]
    timestamp: datetime
    improvement_achieved: float

class BaserahFeedbackMediatorSystem:
    """
    Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ· Ù„Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠ
    ÙŠØªÙˆØ³Ø· Ø¨ÙŠÙ† Ø§Ù„Ø¹ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¨Ø·Ø© ÙˆØ§Ù„Ø±Ø§Ø³Ù… Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø©
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ·."""
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.quality_assessor = None
        self.expert_system = None
        self.inference_engine = None
        self.artistic_renderer = None
        
        # ØªØ§Ø±ÙŠØ® Ø¯ÙˆØ±Ø§Øª Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        self.feedback_cycles = []
        self.learning_rate = 0.05
        self.max_cycles = 10
        self.convergence_threshold = 0.95
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.total_improvements = 0
        self.successful_cycles = 0
        
        print("ðŸ¤– ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ· Ù„Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠ")
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
        self._initialize_components()
    
    def _initialize_components(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©."""
        
        try:
            # Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©
            from .quality_assessment_engine import BaserahQualityAssessmentEngine
            self.quality_assessor = BaserahQualityAssessmentEngine()
            print("   âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©")
            
            # Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø·
            from .inference_engine import BaserahInferenceEngine
            self.inference_engine = BaserahInferenceEngine()
            print("   âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø·")
            
            # Ø§Ù„Ø±Ø§Ø³Ù… Ø§Ù„ÙÙ†ÙŠ
            from .artistic_renderer import BaserahArtisticRenderer
            self.artistic_renderer = BaserahArtisticRenderer()
            print("   âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø±Ø§Ø³Ù… Ø§Ù„ÙÙ†ÙŠ")
            
            # Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            try:
                from revolutionary_intelligence.expert_explorer_system.expert_explorer.integrated_expert_explorer import BaserahIntegratedExpertExplorer
                self.expert_system = BaserahIntegratedExpertExplorer()
                print("   âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±/Ø§Ù„Ù…Ø³ØªÙƒØ´Ù")
            except ImportError:
                print("   âš ï¸ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ± ØºÙŠØ± Ù…ØªØ§Ø­ - Ø³ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø¯ÙˆÙ†Ù‡")
            
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª: {e}")
    
    def run_feedback_cycle(self, original_data: List[Tuple[float, float]], 
                          initial_confidence: float = 0.5) -> Dict[str, Any]:
        """
        ØªØ´ØºÙŠÙ„ Ø¯ÙˆØ±Ø© ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© ÙƒØ§Ù…Ù„Ø©.
        
        Args:
            original_data: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
            initial_confidence: Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
            
        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        """
        
        cycle_id = f"feedback_cycle_{uuid.uuid4()}"
        print(f"\nðŸ”„ Ø¨Ø¯Ø¡ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©: {cycle_id}")
        print(f"ðŸ“Š Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©: {len(original_data)} Ù†Ù‚Ø·Ø©")
        
        best_result = None
        best_quality = 0.0
        cycle_count = 0
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø·
        x_data = [p[0] for p in original_data]
        y_data = [p[1] for p in original_data]
        
        current_confidence = initial_confidence
        
        while cycle_count < self.max_cycles:
            cycle_count += 1
            print(f"\nðŸ”„ Ø§Ù„Ø¯ÙˆØ±Ø© {cycle_count}/{self.max_cycles}")
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø·
            inferred_equations = self._perform_inference(x_data, y_data, current_confidence)
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªÙˆÙ„ÙŠØ¯
            generated_data = self._generate_from_equations(inferred_equations, x_data)
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
            quality_metrics = self._assess_quality(original_data, generated_data)
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±
            if self.expert_system:
                enhanced_equations = self._enhance_with_expert_system(
                    inferred_equations, quality_metrics, original_data
                )
                
                # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø©
                enhanced_generated = self._generate_from_equations(enhanced_equations, x_data)
                enhanced_quality = self._assess_quality(original_data, enhanced_generated)
                
                # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„
                if enhanced_quality.overall_score > quality_metrics.overall_score:
                    inferred_equations = enhanced_equations
                    generated_data = enhanced_generated
                    quality_metrics = enhanced_quality
                    print("   ðŸ§  ØªÙ… ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±")
            
            # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
            feedback = self.quality_assessor.generate_feedback(quality_metrics)
            
            # Ø­ÙØ¸ Ø§Ù„Ø¯ÙˆØ±Ø©
            cycle = FeedbackCycle(
                cycle_id=f"{cycle_id}_iteration_{cycle_count}",
                original_data=original_data,
                inferred_equations=inferred_equations,
                generated_data=generated_data,
                quality_metrics=quality_metrics,
                feedback=feedback,
                timestamp=datetime.now(),
                improvement_achieved=quality_metrics.overall_score - best_quality
            )
            
            self.feedback_cycles.append(cycle)
            
            # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
            if quality_metrics.overall_score > best_quality:
                best_quality = quality_metrics.overall_score
                best_result = cycle
                print(f"   ðŸ“ˆ ØªØ­Ø³Ù† Ø§Ù„Ø¬ÙˆØ¯Ø©: {quality_metrics.overall_score:.3f}")
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø«Ù‚Ø©
            current_confidence += feedback.get('confidence_adjustment', 0.0)
            current_confidence = max(0.1, min(0.99, current_confidence))
            
            # ÙØ­Øµ Ø§Ù„ØªÙ‚Ø§Ø±Ø¨
            if quality_metrics.overall_score >= self.convergence_threshold:
                print(f"   ðŸŽ¯ ØªÙ… Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„ØªÙ‚Ø§Ø±Ø¨ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {quality_metrics.overall_score:.3f}")
                self.successful_cycles += 1
                break
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ù„Ù„Ø¯ÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            if cycle_count < self.max_cycles:
                self._apply_feedback_improvements(feedback)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_improvement = best_quality - initial_confidence
        self.total_improvements += final_improvement
        
        result = {
            'cycle_id': cycle_id,
            'total_iterations': cycle_count,
            'best_quality': best_quality,
            'final_confidence': current_confidence,
            'improvement_achieved': final_improvement,
            'converged': best_quality >= self.convergence_threshold,
            'best_equations': best_result.inferred_equations if best_result else None,
            'best_generated_data': best_result.generated_data if best_result else None,
            'feedback_summary': best_result.feedback if best_result else None
        }
        
        print(f"\nðŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø¯ÙˆØ±Ø© Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©:")
        print(f"   ðŸŽ¯ Ø£ÙØ¶Ù„ Ø¬ÙˆØ¯Ø©: {best_quality:.3f}")
        print(f"   ðŸ“ˆ Ø§Ù„ØªØ­Ø³Ù†: +{final_improvement:.3f}")
        print(f"   ðŸ”„ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {cycle_count}")
        print(f"   âœ… Ø§Ù„ØªÙ‚Ø§Ø±Ø¨: {'Ù†Ø¹Ù…' if result['converged'] else 'Ù„Ø§'}")
        
        return result
    
    def _perform_inference(self, x_data: List[float], y_data: List[float], 
                          confidence: float) -> Dict[str, Any]:
        """ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø· Ù…Ø¹ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø­Ø¯Ø«Ø©."""
        
        if not self.inference_engine:
            return {'error': 'Ù…Ø­Ø±Ùƒ Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø· ØºÙŠØ± Ù…ØªØ§Ø­'}
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªÙ†Ø¨Ø§Ø·
        result = self.inference_engine.infer_from_data_points(x_data, y_data)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø«Ù‚Ø©
        if 'confidence' in result:
            result['confidence'] = min(0.99, result['confidence'] + confidence * self.learning_rate)
        
        return result
    
    def _generate_from_equations(self, equations: Dict[str, Any], 
                               x_data: List[float]) -> List[Tuple[float, float]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª."""
        
        if not self.artistic_renderer or 'error' in equations:
            # ØªÙˆÙ„ÙŠØ¯ Ø¨Ø³ÙŠØ· ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ø±Ø§Ø³Ù…
            return [(x, 0.0) for x in x_data]
        
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø§Ø³Ù… Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            generated_points = []
            
            for x in x_data:
                # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¨Ø·Ø©
                if equations.get('type') == 'linear':
                    components = equations.get('components', [])
                    y = 0.0
                    
                    for comp in components:
                        if comp.get('type') == 'linear':
                            params = comp.get('parameters', {})
                            beta = params.get('beta', 1.0)
                            gamma = params.get('gamma', 0.0)
                            
                            from .baserah_core import baserah_linear
                            y += baserah_linear(x, beta=beta, gamma=gamma)
                
                elif equations.get('type') == 'sigmoid':
                    components = equations.get('components', [])
                    y = 0.0
                    
                    for comp in components:
                        if comp.get('type') == 'sigmoid':
                            params = comp.get('parameters', {})
                            n = params.get('n', 1)
                            k = params.get('k', 1.0)
                            x0 = params.get('x0', 0.0)
                            alpha = params.get('alpha', 1.0)
                            
                            from .baserah_core import baserah_sigmoid
                            y += baserah_sigmoid(x, n=n, k=k, x0=x0, alpha=alpha)
                
                else:
                    # Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ·
                    y = x
                
                generated_points.append((x, y))
            
            return generated_points
            
        except Exception as e:
            print(f"   âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯: {e}")
            return [(x, 0.0) for x in x_data]
    
    def _assess_quality(self, original: List[Tuple[float, float]], 
                       generated: List[Tuple[float, float]]):
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©."""
        
        if not self.quality_assessor:
            # ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³ÙŠØ· ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ØªÙˆÙØ± Ø§Ù„Ù…Ù‚ÙŠÙ…
            class SimpleMetrics:
                def __init__(self):
                    self.overall_score = 0.5
                    self.structural_similarity = 0.5
                    self.mathematical_accuracy = 0.5
                    self.visual_fidelity = 0.5
            
            return SimpleMetrics()
        
        return self.quality_assessor.assess_quality(original, generated)
    
    def _enhance_with_expert_system(self, equations: Dict[str, Any], 
                                  quality_metrics: Any,
                                  original_data: List[Tuple[float, float]]) -> Dict[str, Any]:
        """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±."""
        
        if not self.expert_system:
            return equations
        
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±
            x_data = [p[0] for p in original_data]
            y_data = [p[1] for p in original_data]
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ø¯Ù„Ø© Ù…ØªÙƒÙŠÙØ©
            adaptive_eq = self.expert_system.create_adaptive_equation_from_data(x_data, y_data)
            
            if adaptive_eq:
                # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ù…ØªÙƒÙŠÙØ© Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
                enhanced_equations = equations.copy()
                enhanced_equations['adaptive_component'] = {
                    'type': 'adaptive',
                    'equation_id': adaptive_eq.id,
                    'components': [comp.__dict__ for comp in adaptive_eq.components]
                }
                
                return enhanced_equations
        
        except Exception as e:
            print(f"   âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø¨Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø¨ÙŠØ±: {e}")
        
        return equations
    
    def _apply_feedback_improvements(self, feedback: Dict[str, Any]):
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©."""
        
        # ØªØ­Ø¯ÙŠØ« Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù…
        if 'confidence_adjustment' in feedback:
            adjustment = feedback['confidence_adjustment']
            if adjustment > 0:
                self.learning_rate = min(0.1, self.learning_rate * 1.1)
            else:
                self.learning_rate = max(0.01, self.learning_rate * 0.9)
        
        # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ù…Ø­Ø¯Ø¯Ø©
        specific_adjustments = feedback.get('specific_adjustments', {})
        for area, adjustment in specific_adjustments.items():
            print(f"   ðŸ”§ ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ† {area}: {adjustment}")
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆØ³ÙŠØ·."""
        
        if not self.feedback_cycles:
            return {'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¯ÙˆØ±Ø§Øª ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø¹Ø¯'}
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        total_cycles = len(self.feedback_cycles)
        quality_scores = [cycle.quality_metrics.overall_score for cycle in self.feedback_cycles]
        improvements = [cycle.improvement_achieved for cycle in self.feedback_cycles]
        
        summary = {
            'total_feedback_cycles': total_cycles,
            'successful_cycles': self.successful_cycles,
            'success_rate': self.successful_cycles / total_cycles if total_cycles > 0 else 0,
            'average_quality': sum(quality_scores) / len(quality_scores),
            'best_quality': max(quality_scores),
            'total_improvement': sum(improvements),
            'average_improvement': sum(improvements) / len(improvements),
            'current_learning_rate': self.learning_rate,
            'convergence_rate': self.successful_cycles / total_cycles if total_cycles > 0 else 0
        }
        
        return summary

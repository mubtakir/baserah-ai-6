#!/usr/bin/env python3
# quality_assessment_engine.py - Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Ù„Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©

import numpy as np
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
import math

@dataclass
class QualityMetrics:
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©."""
    structural_similarity: float
    mathematical_accuracy: float
    visual_fidelity: float
    overall_score: float
    error_analysis: Dict[str, Any]

class BaserahQualityAssessmentEngine:
    """
    Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠ
    ÙŠÙ‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ø°ÙƒÙŠØ©
    """
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©."""
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        self.structural_weight = 0.4
        self.mathematical_weight = 0.3
        self.visual_weight = 0.3
        
        # Ø¹ØªØ¨Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©
        self.excellent_threshold = 0.9
        self.good_threshold = 0.7
        self.acceptable_threshold = 0.5
        
        # ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
        self.assessment_history = []
        
        print("ğŸ” ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠ")
    
    def assess_quality(self, original_data: List[Tuple[float, float]], 
                      generated_data: List[Tuple[float, float]]) -> QualityMetrics:
        """
        ØªÙ‚ÙŠÙŠÙ… Ø´Ø§Ù…Ù„ Ù„Ù„Ø¬ÙˆØ¯Ø© Ø¨ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© ÙˆØ§Ù„Ù…ÙˆÙ„Ø¯Ø©.
        
        Args:
            original_data: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© [(x, y), ...]
            generated_data: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© [(x, y), ...]
            
        Returns:
            QualityMetrics: Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
        """
        
        print(f"ğŸ” Ø¨Ø¯Ø¡ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¬ÙˆØ¯Ø©: {len(original_data)} Ù†Ù‚Ø·Ø© Ø£ØµÙ„ÙŠØ©ØŒ {len(generated_data)} Ù†Ù‚Ø·Ø© Ù…ÙˆÙ„Ø¯Ø©")
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ
        structural_sim = self._calculate_structural_similarity(original_data, generated_data)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©
        math_accuracy = self._calculate_mathematical_accuracy(original_data, generated_data)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¥Ø®Ù„Ø§Øµ Ø§Ù„Ø¨ØµØ±ÙŠ
        visual_fidelity = self._calculate_visual_fidelity(original_data, generated_data)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_score = (
            self.structural_weight * structural_sim +
            self.mathematical_weight * math_accuracy +
            self.visual_weight * visual_fidelity
        )
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        error_analysis = self._analyze_errors(original_data, generated_data)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
        metrics = QualityMetrics(
            structural_similarity=structural_sim,
            mathematical_accuracy=math_accuracy,
            visual_fidelity=visual_fidelity,
            overall_score=overall_score,
            error_analysis=error_analysis
        )
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
        self.assessment_history.append(metrics)
        
        print(f"ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…:")
        print(f"   Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ: {structural_sim:.3f}")
        print(f"   Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©: {math_accuracy:.3f}")
        print(f"   Ø§Ù„Ø¥Ø®Ù„Ø§Øµ Ø§Ù„Ø¨ØµØ±ÙŠ: {visual_fidelity:.3f}")
        print(f"   Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {overall_score:.3f}")
        
        return metrics
    
    def _calculate_structural_similarity(self, original: List[Tuple[float, float]], 
                                       generated: List[Tuple[float, float]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙˆØ§Ù„ Baserah Ø§Ù„Ù†Ù‚ÙŠØ©."""
        
        if not original or not generated:
            return 0.0
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª
        orig_x = [p[0] for p in original]
        orig_y = [p[1] for p in original]
        gen_x = [p[0] for p in generated]
        gen_y = [p[1] for p in generated]
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        orig_mean_x = sum(orig_x) / len(orig_x)
        orig_mean_y = sum(orig_y) / len(orig_y)
        gen_mean_x = sum(gen_x) / len(gen_x)
        gen_mean_y = sum(gen_y) / len(gen_y)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ†
        orig_var_x = sum((x - orig_mean_x) ** 2 for x in orig_x) / len(orig_x)
        orig_var_y = sum((y - orig_mean_y) ** 2 for y in orig_y) / len(orig_y)
        gen_var_x = sum((x - gen_mean_x) ** 2 for x in gen_x) / len(gen_x)
        gen_var_y = sum((y - gen_mean_y) ** 2 for y in gen_y) / len(gen_y)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
        from .baserah_core import baserah_sigmoid
        
        mean_diff = abs(orig_mean_x - gen_mean_x) + abs(orig_mean_y - gen_mean_y)
        var_diff = abs(orig_var_x - gen_var_x) + abs(orig_var_y - gen_var_y)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· ØªØ´Ø§Ø¨Ù‡
        mean_similarity = baserah_sigmoid(-mean_diff, n=1, k=2.0, x0=0.0, alpha=1.0)
        var_similarity = baserah_sigmoid(-var_diff, n=1, k=1.0, x0=0.0, alpha=1.0)
        
        structural_similarity = (mean_similarity + var_similarity) / 2
        
        return structural_similarity
    
    def _calculate_mathematical_accuracy(self, original: List[Tuple[float, float]], 
                                       generated: List[Tuple[float, float]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Baserah."""
        
        if not original or not generated:
            return 0.0
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ø¥Ù‚Ù„ÙŠØ¯ÙŠØ©
        total_distance = 0.0
        min_len = min(len(original), len(generated))
        
        for i in range(min_len):
            orig_x, orig_y = original[i]
            gen_x, gen_y = generated[i]
            
            distance = math.sqrt((orig_x - gen_x) ** 2 + (orig_y - gen_y) ** 2)
            total_distance += distance
        
        avg_distance = total_distance / min_len if min_len > 0 else float('inf')
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø¯Ù‚Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
        from .baserah_core import baserah_sigmoid
        
        # ÙƒÙ„Ù…Ø§ Ù‚Ù„Øª Ø§Ù„Ù…Ø³Ø§ÙØ©ØŒ Ø²Ø§Ø¯Øª Ø§Ù„Ø¯Ù‚Ø©
        accuracy = baserah_sigmoid(-avg_distance, n=1, k=3.0, x0=0.0, alpha=1.0)
        
        return accuracy
    
    def _calculate_visual_fidelity(self, original: List[Tuple[float, float]], 
                                 generated: List[Tuple[float, float]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø®Ù„Ø§Øµ Ø§Ù„Ø¨ØµØ±ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´ÙƒÙ„."""
        
        if not original or not generated:
            return 0.0
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¹Ø§Ù…
        orig_shape_features = self._extract_shape_features(original)
        gen_shape_features = self._extract_shape_features(generated)
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø®ØµØ§Ø¦Øµ
        feature_similarity = 0.0
        feature_count = 0
        
        for feature_name in orig_shape_features:
            if feature_name in gen_shape_features:
                orig_val = orig_shape_features[feature_name]
                gen_val = gen_shape_features[feature_name]
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø®Ø·ÙŠØ©
                from .baserah_core import baserah_linear
                
                diff = abs(orig_val - gen_val)
                similarity = max(0, baserah_linear(-diff, beta=1.0, gamma=1.0))
                feature_similarity += similarity
                feature_count += 1
        
        visual_fidelity = feature_similarity / feature_count if feature_count > 0 else 0.0
        
        return min(visual_fidelity, 1.0)
    
    def _extract_shape_features(self, data: List[Tuple[float, float]]) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø´ÙƒÙ„."""
        
        if len(data) < 3:
            return {}
        
        x_coords = [p[0] for p in data]
        y_coords = [p[1] for p in data]
        
        features = {
            'center_x': sum(x_coords) / len(x_coords),
            'center_y': sum(y_coords) / len(y_coords),
            'range_x': max(x_coords) - min(x_coords),
            'range_y': max(y_coords) - min(y_coords),
            'aspect_ratio': (max(x_coords) - min(x_coords)) / (max(y_coords) - min(y_coords) + 1e-6),
            'perimeter': self._calculate_perimeter(data),
            'compactness': self._calculate_compactness(data)
        }
        
        return features
    
    def _calculate_perimeter(self, data: List[Tuple[float, float]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø­ÙŠØ·."""
        
        if len(data) < 2:
            return 0.0
        
        perimeter = 0.0
        for i in range(len(data)):
            x1, y1 = data[i]
            x2, y2 = data[(i + 1) % len(data)]
            perimeter += math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)
        
        return perimeter
    
    def _calculate_compactness(self, data: List[Tuple[float, float]]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†Ø¶ØºØ§Ø·."""
        
        perimeter = self._calculate_perimeter(data)
        if perimeter == 0:
            return 0.0
        
        # ØªÙ‚Ø¯ÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© Ø§Ù„Ø­Ø°Ø§Ø¡
        area = 0.0
        for i in range(len(data)):
            x1, y1 = data[i]
            x2, y2 = data[(i + 1) % len(data)]
            area += x1 * y2 - x2 * y1
        area = abs(area) / 2.0
        
        # Ø§Ù„Ø§Ù†Ø¶ØºØ§Ø· = 4Ï€ Ã— Ø§Ù„Ù…Ø³Ø§Ø­Ø© / Ø§Ù„Ù…Ø­ÙŠØ·Â²
        compactness = (4 * math.pi * area) / (perimeter ** 2) if perimeter > 0 else 0.0
        
        return compactness
    
    def _analyze_errors(self, original: List[Tuple[float, float]], 
                       generated: List[Tuple[float, float]]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡."""
        
        error_analysis = {
            'total_points_original': len(original),
            'total_points_generated': len(generated),
            'point_count_difference': abs(len(original) - len(generated)),
            'error_types': [],
            'recommendations': []
        }
        
        if len(original) != len(generated):
            error_analysis['error_types'].append('point_count_mismatch')
            error_analysis['recommendations'].append('Ø¶Ø¨Ø· Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©')
        
        if original and generated:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ù‚Ø·ÙŠØ©
            min_len = min(len(original), len(generated))
            point_errors = []
            
            for i in range(min_len):
                orig_x, orig_y = original[i]
                gen_x, gen_y = generated[i]
                
                error = math.sqrt((orig_x - gen_x) ** 2 + (orig_y - gen_y) ** 2)
                point_errors.append(error)
            
            if point_errors:
                error_analysis['mean_point_error'] = sum(point_errors) / len(point_errors)
                error_analysis['max_point_error'] = max(point_errors)
                error_analysis['min_point_error'] = min(point_errors)
                
                # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
                high_error_count = sum(1 for e in point_errors if e > 0.5)
                if high_error_count > len(point_errors) * 0.3:
                    error_analysis['error_types'].append('high_point_errors')
                    error_analysis['recommendations'].append('ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª')
        
        return error_analysis
    
    def generate_feedback(self, metrics: QualityMetrics) -> Dict[str, Any]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØºØ°ÙŠØ© Ø±Ø§Ø¬Ø¹Ø© Ø°ÙƒÙŠØ© Ù…Ù† Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©."""
        
        feedback = {
            'overall_quality': self._classify_quality(metrics.overall_score),
            'improvement_areas': [],
            'specific_adjustments': {},
            'confidence_adjustment': 0.0,
            'priority_actions': []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù
        if metrics.structural_similarity < 0.7:
            feedback['improvement_areas'].append('structural_similarity')
            feedback['specific_adjustments']['structure'] = 'ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ'
            feedback['priority_actions'].append('Ø¶Ø¨Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ø¹Ø§Ù…')
        
        if metrics.mathematical_accuracy < 0.7:
            feedback['improvement_areas'].append('mathematical_accuracy')
            feedback['specific_adjustments']['mathematics'] = 'ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ©'
            feedback['priority_actions'].append('Ø¶Ø¨Ø· Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª')
        
        if metrics.visual_fidelity < 0.7:
            feedback['improvement_areas'].append('visual_fidelity')
            feedback['specific_adjustments']['visual'] = 'ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¥Ø®Ù„Ø§Øµ Ø§Ù„Ø¨ØµØ±ÙŠ'
            feedback['priority_actions'].append('Ø¶Ø¨Ø· Ø®ØµØ§Ø¦Øµ Ø§Ù„Ø±Ø³Ù…')
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©
        if metrics.overall_score > 0.9:
            feedback['confidence_adjustment'] = 0.1
        elif metrics.overall_score > 0.7:
            feedback['confidence_adjustment'] = 0.05
        elif metrics.overall_score < 0.5:
            feedback['confidence_adjustment'] = -0.2
        else:
            feedback['confidence_adjustment'] = -0.1
        
        return feedback
    
    def _classify_quality(self, score: float) -> str:
        """ØªØµÙ†ÙŠÙ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø¬ÙˆØ¯Ø©."""
        
        if score >= self.excellent_threshold:
            return 'Ù…Ù…ØªØ§Ø²'
        elif score >= self.good_threshold:
            return 'Ø¬ÙŠØ¯'
        elif score >= self.acceptable_threshold:
            return 'Ù…Ù‚Ø¨ÙˆÙ„'
        else:
            return 'ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†'
    
    def get_assessment_summary(self) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø§Ù„Ø¬ÙˆØ¯Ø©."""
        
        if not self.assessment_history:
            return {'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙ‚ÙŠÙŠÙ…Ø§Øª Ø¨Ø¹Ø¯'}
        
        scores = [m.overall_score for m in self.assessment_history]
        
        summary = {
            'total_assessments': len(self.assessment_history),
            'average_score': sum(scores) / len(scores),
            'best_score': max(scores),
            'worst_score': min(scores),
            'improvement_trend': self._calculate_trend(scores),
            'recent_performance': scores[-5:] if len(scores) >= 5 else scores
        }
        
        return summary
    
    def _calculate_trend(self, scores: List[float]) -> str:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ­Ø³Ù†."""
        
        if len(scores) < 3:
            return 'ØºÙŠØ± ÙƒØ§ÙÙŠ Ù„Ù„ØªÙ‚ÙŠÙŠÙ…'
        
        recent_avg = sum(scores[-3:]) / 3
        earlier_avg = sum(scores[:-3]) / len(scores[:-3]) if len(scores) > 3 else scores[0]
        
        if recent_avg > earlier_avg + 0.05:
            return 'ØªØ­Ø³Ù†'
        elif recent_avg < earlier_avg - 0.05:
            return 'ØªØ±Ø§Ø¬Ø¹'
        else:
            return 'Ù…Ø³ØªÙ‚Ø±'

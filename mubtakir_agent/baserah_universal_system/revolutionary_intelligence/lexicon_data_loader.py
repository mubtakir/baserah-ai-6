#!/usr/bin/env python3
# lexicon_data_loader.py - Ù…Ø­Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
# ÙŠØ¯Ø¹Ù… ØªØ­Ù…ÙŠÙ„ Ù„Ø³Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ ÙˆØºÙŠØ±Ù‡ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø¨ØµÙŠØº Ù…Ø®ØªÙ„ÙØ©

import os
import sys
import json
import sqlite3
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import re

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø«ÙˆØ±ÙŠ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from .arabic_lexicon_engine import ArabicLexiconEngine, LexiconSource


@dataclass
class LexiconDataSource:
    """Ù…ØµØ¯Ø± Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹Ø¬Ù…ÙŠ."""
    name: str
    source_type: str  # 'api', 'xml', 'json', 'database', 'text'
    url: Optional[str]
    file_path: Optional[str]
    api_key: Optional[str]
    description: str
    is_active: bool = True


class LexiconDataLoader:
    """
    Ù…Ø­Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø«ÙˆØ±ÙŠ.
    
    ÙŠØ¯Ø¹Ù… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ù…Ù†:
    1. APIs Ø¹Ø±Ø¨ÙŠØ© (Ø§Ù„Ø¨Ø§Ø­Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØŒ ÙƒÙ„Ù…Ø§ØªØŒ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ)
    2. Ù…Ù„ÙØ§Øª XML (Ù„Ø³Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ØŒ ØªØ§Ø¬ Ø§Ù„Ø¹Ø±ÙˆØ³)
    3. Ù…Ù„ÙØ§Øª JSON (Ù…Ø¹Ø§Ø¬Ù… Ø±Ù‚Ù…ÙŠØ©)
    4. Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª (SQLite, MySQL)
    5. Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ù…Ù†Ø¸Ù…Ø©
    """
    
    def __init__(self, lexicon_engine: ArabicLexiconEngine):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
        
        self.lexicon_engine = lexicon_engine
        self.creation_time = datetime.now()
        
        # Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
        self.data_sources = {
            'almaany_api': LexiconDataSource(
                name="Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ",
                source_type="api",
                url="https://www.almaany.com/api/",
                file_path=None,
                api_key=None,
                description="API Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ù„Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"
            ),
            'baheth_arabi': LexiconDataSource(
                name="Ø§Ù„Ø¨Ø§Ø­Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠ",
                source_type="api",
                url="https://www.baheth.info/api/",
                file_path=None,
                api_key=None,
                description="API Ø§Ù„Ø¨Ø§Ø­Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù„Ù„ØªØ±Ø§Ø«"
            ),
            'lisan_xml': LexiconDataSource(
                name="Ù„Ø³Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ XML",
                source_type="xml",
                url=None,
                file_path="data/lexicons/lisan_al_arab.xml",
                api_key=None,
                description="Ù…Ù„Ù XML Ù„Ù„Ø³Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨"
            ),
            'taj_xml': LexiconDataSource(
                name="ØªØ§Ø¬ Ø§Ù„Ø¹Ø±ÙˆØ³ XML",
                source_type="xml",
                url=None,
                file_path="data/lexicons/taj_al_arous.xml",
                api_key=None,
                description="Ù…Ù„Ù XML Ù„ØªØ§Ø¬ Ø§Ù„Ø¹Ø±ÙˆØ³"
            ),
            'custom_json': LexiconDataSource(
                name="Ù…Ø¹Ø¬Ù… JSON Ù…Ø®ØµØµ",
                source_type="json",
                url=None,
                file_path="data/lexicons/custom_lexicon.json",
                api_key=None,
                description="Ù…Ù„Ù JSON Ù„Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…Ø®ØµØµ"
            ),
            'text_format': LexiconDataSource(
                name="Ù…Ø¹Ø¬Ù… Ù†ØµÙŠ Ù…Ù†Ø¸Ù…",
                source_type="text",
                url=None,
                file_path="data/lexicons/organized_lexicon.txt",
                api_key=None,
                description="Ù…Ù„Ù Ù†ØµÙŠ Ù…Ù†Ø¸Ù… Ù„Ù„Ù…Ø¹Ø¬Ù…"
            )
        }
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
        self.loading_stats = {
            'sources_loaded': 0,
            'total_entries_loaded': 0,
            'successful_loads': 0,
            'failed_loads': 0,
            'last_load_time': None
        }
        
        print(f"ğŸ“¥ ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¬Ù…")
        print(f"ğŸ”— Ù…ØµØ§Ø¯Ø± Ù…ØªØ§Ø­Ø©: {len(self.data_sources)}")
    
    def load_from_api(self, source_name: str, word_list: List[str] = None) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† API."""
        
        if source_name not in self.data_sources:
            return {'success': False, 'error': f'Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {source_name}'}
        
        source = self.data_sources[source_name]
        if source.source_type != 'api':
            return {'success': False, 'error': f'Ø§Ù„Ù…ØµØ¯Ø± Ù„ÙŠØ³ API: {source_name}'}
        
        print(f"ğŸŒ ØªØ­Ù…ÙŠÙ„ Ù…Ù† API: {source.name}")
        
        loaded_entries = []
        
        # Ù‚Ø§Ø¦Ù…Ø© ÙƒÙ„Ù…Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        if word_list is None:
            word_list = ['Ø§Ù„Ù„Ù‡', 'Ø­ÙŠØ§Ø©', 'Ø¹Ù„Ù…', 'Ø­ÙƒÙ…Ø©', 'Ø³Ù„Ø§Ù…', 'Ù†ÙˆØ±', 'ÙƒØªØ§Ø¨', 'Ù‚Ù„Ù…', 'Ø¨ÙŠØª', 'Ù…Ø§Ø¡']
        
        for word in word_list:
            try:
                # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API (ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµÙ‡ Ù„ÙƒÙ„ API)
                api_result = self._simulate_api_call(source, word)
                
                if api_result['success']:
                    entry_data = {
                        'word': word,
                        'root': api_result.get('root', ''),
                        'meaning': api_result.get('meaning', ''),
                        'detailed_meaning': api_result.get('detailed_meaning', ''),
                        'usage_examples': api_result.get('examples', []),
                        'source': source_name
                    }
                    
                    loaded_entries.append(entry_data)
                    
                    # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ
                    self._save_entry_to_engine(entry_data, LexiconSource.CUSTOM_DATABASE)
                
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ ÙƒÙ„Ù…Ø© '{word}' Ù…Ù† {source.name}: {e}")
        
        self.loading_stats['total_entries_loaded'] += len(loaded_entries)
        self.loading_stats['successful_loads'] += 1
        self.loading_stats['last_load_time'] = datetime.now().isoformat()
        
        return {
            'success': True,
            'source': source.name,
            'entries_loaded': len(loaded_entries),
            'entries': loaded_entries
        }
    
    def _simulate_api_call(self, source: LexiconDataSource, word: str) -> Dict[str, Any]:
        """Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ API (Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±)."""
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª API
        simulated_data = {
            'Ø§Ù„Ù„Ù‡': {
                'root': 'Ø£Ù„Ù‡',
                'meaning': 'Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯',
                'detailed_meaning': 'Ø§Ø³Ù… Ø§Ù„Ø¬Ù„Ø§Ù„Ø©ØŒ Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„Ù…Ø¹Ø¨ÙˆØ¯ Ø¨Ø­Ù‚ØŒ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯ Ø§Ù„ÙØ±Ø¯ Ø§Ù„ØµÙ…Ø¯',
                'examples': ['Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰', 'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…']
            },
            'Ø­ÙŠØ§Ø©': {
                'root': 'Ø­ÙŠÙŠ',
                'meaning': 'Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¨Ù‚Ø§Ø¡',
                'detailed_meaning': 'Ø§Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„Ø­Ø±ÙƒØ© ÙˆØ§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ù„ÙˆØ¬ÙˆØ¯ØŒ Ø¶Ø¯ Ø§Ù„Ù…ÙˆØª',
                'examples': ['Ø§Ù„Ø­ÙŠØ§Ø© Ø¬Ù…ÙŠÙ„Ø©', 'Ø­ÙŠØ§Ø© ÙƒØ±ÙŠÙ…Ø©']
            },
            'Ø¹Ù„Ù…': {
                'root': 'Ø¹Ù„Ù…',
                'meaning': 'Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ',
                'detailed_meaning': 'Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ø´ÙŠØ¡ Ø¹Ù„Ù‰ Ù…Ø§ Ù‡Ùˆ Ø¹Ù„ÙŠÙ‡ Ø¥Ø¯Ø±Ø§ÙƒØ§Ù‹ Ø¬Ø§Ø²Ù…Ø§Ù‹',
                'examples': ['Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù„Ù… ÙØ±ÙŠØ¶Ø©', 'Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±']
            }
        }
        
        if word in simulated_data:
            result = simulated_data[word].copy()
            result['success'] = True
            return result
        else:
            return {
                'success': True,
                'root': word[:3] if len(word) >= 3 else word,
                'meaning': f'Ù…Ø¹Ù†Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø©: {word}',
                'detailed_meaning': f'ØªÙØ³ÙŠØ± Ù…ÙØµÙ„ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}',
                'examples': [f'Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… {word}']
            }
    
    def load_from_xml(self, source_name: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù XML."""
        
        if source_name not in self.data_sources:
            return {'success': False, 'error': f'Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {source_name}'}
        
        source = self.data_sources[source_name]
        if source.source_type != 'xml':
            return {'success': False, 'error': f'Ø§Ù„Ù…ØµØ¯Ø± Ù„ÙŠØ³ XML: {source_name}'}
        
        if not source.file_path or not os.path.exists(source.file_path):
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù XML ØªØ¬Ø±ÙŠØ¨ÙŠ
            return self._create_sample_xml(source)
        
        print(f"ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ù…Ù† XML: {source.name}")
        
        try:
            tree = ET.parse(source.file_path)
            root = tree.getroot()
            
            loaded_entries = []
            
            # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ XML (ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµÙ‡ Ø­Ø³Ø¨ Ù‡ÙŠÙƒÙ„ ÙƒÙ„ Ù…Ø¹Ø¬Ù…)
            for entry in root.findall('.//entry'):
                word = entry.find('word')
                meaning = entry.find('meaning')
                root_elem = entry.find('root')
                
                if word is not None and meaning is not None:
                    entry_data = {
                        'word': word.text.strip(),
                        'root': root_elem.text.strip() if root_elem is not None else '',
                        'meaning': meaning.text.strip(),
                        'detailed_meaning': meaning.text.strip(),
                        'usage_examples': [],
                        'source': source_name
                    }
                    
                    loaded_entries.append(entry_data)
                    self._save_entry_to_engine(entry_data, LexiconSource.LISAN_AL_ARAB)
            
            self.loading_stats['total_entries_loaded'] += len(loaded_entries)
            self.loading_stats['successful_loads'] += 1
            
            return {
                'success': True,
                'source': source.name,
                'entries_loaded': len(loaded_entries),
                'entries': loaded_entries[:10]  # Ø£ÙˆÙ„ 10 Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ø¹Ø±Ø¶
            }
            
        except Exception as e:
            self.loading_stats['failed_loads'] += 1
            return {'success': False, 'error': f'Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ XML: {e}'}
    
    def _create_sample_xml(self, source: LexiconDataSource) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù XML ØªØ¬Ø±ÙŠØ¨ÙŠ."""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        os.makedirs(os.path.dirname(source.file_path), exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ XML ØªØ¬Ø±ÙŠØ¨ÙŠ
        root = ET.Element("lexicon")
        root.set("name", source.name)
        root.set("created", datetime.now().isoformat())
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¯Ø®Ù„Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
        sample_entries = [
            {'word': 'Ø§Ù„Ù„Ù‡', 'root': 'Ø£Ù„Ù‡', 'meaning': 'Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯'},
            {'word': 'Ø­ÙŠØ§Ø©', 'root': 'Ø­ÙŠÙŠ', 'meaning': 'Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¨Ù‚Ø§Ø¡'},
            {'word': 'Ø¹Ù„Ù…', 'root': 'Ø¹Ù„Ù…', 'meaning': 'Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ'},
            {'word': 'Ø­ÙƒÙ…Ø©', 'root': 'Ø­ÙƒÙ…', 'meaning': 'Ø§Ù„Ø¹Ø¯Ù„ ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù†'},
            {'word': 'Ø³Ù„Ø§Ù…', 'root': 'Ø³Ù„Ù…', 'meaning': 'Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø©'}
        ]
        
        for entry_data in sample_entries:
            entry = ET.SubElement(root, "entry")
            
            word = ET.SubElement(entry, "word")
            word.text = entry_data['word']
            
            root_elem = ET.SubElement(entry, "root")
            root_elem.text = entry_data['root']
            
            meaning = ET.SubElement(entry, "meaning")
            meaning.text = entry_data['meaning']
            
            detailed = ET.SubElement(entry, "detailed_meaning")
            detailed.text = f"ØªÙØ³ÙŠØ± Ù…ÙØµÙ„ Ù„ÙƒÙ„Ù…Ø© {entry_data['word']}: {entry_data['meaning']}"
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        tree = ET.ElementTree(root)
        tree.write(source.file_path, encoding='utf-8', xml_declaration=True)
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù XML ØªØ¬Ø±ÙŠØ¨ÙŠ: {source.file_path}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        return self.load_from_xml(source.name.split()[0].lower() + '_xml')
    
    def load_from_json(self, source_name: str) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ù…Ù„Ù JSON."""
        
        if source_name not in self.data_sources:
            return {'success': False, 'error': f'Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ: {source_name}'}
        
        source = self.data_sources[source_name]
        if source.source_type != 'json':
            return {'success': False, 'error': f'Ø§Ù„Ù…ØµØ¯Ø± Ù„ÙŠØ³ JSON: {source_name}'}
        
        if not source.file_path or not os.path.exists(source.file_path):
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù JSON ØªØ¬Ø±ÙŠØ¨ÙŠ
            return self._create_sample_json(source)
        
        print(f"ğŸ“„ ØªØ­Ù…ÙŠÙ„ Ù…Ù† JSON: {source.name}")
        
        try:
            with open(source.file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            loaded_entries = []
            
            # ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ JSON
            entries = data.get('entries', [])
            for entry in entries:
                if 'word' in entry and 'meaning' in entry:
                    entry_data = {
                        'word': entry['word'],
                        'root': entry.get('root', ''),
                        'meaning': entry['meaning'],
                        'detailed_meaning': entry.get('detailed_meaning', entry['meaning']),
                        'usage_examples': entry.get('examples', []),
                        'source': source_name
                    }
                    
                    loaded_entries.append(entry_data)
                    self._save_entry_to_engine(entry_data, LexiconSource.CUSTOM_DATABASE)
            
            self.loading_stats['total_entries_loaded'] += len(loaded_entries)
            self.loading_stats['successful_loads'] += 1
            
            return {
                'success': True,
                'source': source.name,
                'entries_loaded': len(loaded_entries),
                'entries': loaded_entries[:10]
            }
            
        except Exception as e:
            self.loading_stats['failed_loads'] += 1
            return {'success': False, 'error': f'Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ JSON: {e}'}
    
    def _create_sample_json(self, source: LexiconDataSource) -> Dict[str, Any]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù JSON ØªØ¬Ø±ÙŠØ¨ÙŠ."""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        os.makedirs(os.path.dirname(source.file_path), exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ JSON ØªØ¬Ø±ÙŠØ¨ÙŠ
        sample_data = {
            "lexicon_info": {
                "name": source.name,
                "created": datetime.now().isoformat(),
                "description": "Ù…Ø¹Ø¬Ù… JSON ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø«ÙˆØ±ÙŠ"
            },
            "entries": [
                {
                    "word": "Ø§Ù„Ù„Ù‡",
                    "root": "Ø£Ù„Ù‡",
                    "meaning": "Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯",
                    "detailed_meaning": "Ø§Ø³Ù… Ø§Ù„Ø¬Ù„Ø§Ù„Ø©ØŒ Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„Ù…Ø¹Ø¨ÙˆØ¯ Ø¨Ø­Ù‚ØŒ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯ Ø§Ù„ÙØ±Ø¯ Ø§Ù„ØµÙ…Ø¯",
                    "examples": ["Ù‚Ø§Ù„ Ø§Ù„Ù„Ù‡ ØªØ¹Ø§Ù„Ù‰", "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…"]
                },
                {
                    "word": "Ø­ÙŠØ§Ø©",
                    "root": "Ø­ÙŠÙŠ",
                    "meaning": "Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¨Ù‚Ø§Ø¡",
                    "detailed_meaning": "Ø§Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„Ø­Ø±ÙƒØ© ÙˆØ§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ù„ÙˆØ¬ÙˆØ¯ØŒ Ø¶Ø¯ Ø§Ù„Ù…ÙˆØª",
                    "examples": ["Ø§Ù„Ø­ÙŠØ§Ø© Ø¬Ù…ÙŠÙ„Ø©", "Ø­ÙŠØ§Ø© ÙƒØ±ÙŠÙ…Ø©"]
                },
                {
                    "word": "Ø¹Ù„Ù…",
                    "root": "Ø¹Ù„Ù…",
                    "meaning": "Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ",
                    "detailed_meaning": "Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ø´ÙŠØ¡ Ø¹Ù„Ù‰ Ù…Ø§ Ù‡Ùˆ Ø¹Ù„ÙŠÙ‡ Ø¥Ø¯Ø±Ø§ÙƒØ§Ù‹ Ø¬Ø§Ø²Ù…Ø§Ù‹",
                    "examples": ["Ø·Ù„Ø¨ Ø§Ù„Ø¹Ù„Ù… ÙØ±ÙŠØ¶Ø©", "Ø§Ù„Ø¹Ù„Ù… Ù†ÙˆØ±"]
                },
                {
                    "word": "Ø­ÙƒÙ…Ø©",
                    "root": "Ø­ÙƒÙ…",
                    "meaning": "Ø§Ù„Ø¹Ø¯Ù„ ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù†",
                    "detailed_meaning": "ÙˆØ¶Ø¹ Ø§Ù„Ø´ÙŠØ¡ ÙÙŠ Ù…ÙˆØ¶Ø¹Ù‡ Ø§Ù„ØµØ­ÙŠØ­ØŒ Ø§Ù„Ø¹Ø¯Ù„ ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù†",
                    "examples": ["Ø§Ù„Ø­ÙƒÙ…Ø© Ø¶Ø§Ù„Ø© Ø§Ù„Ù…Ø¤Ù…Ù†", "Ø­ÙƒÙ…Ø© Ø¨Ø§Ù„ØºØ©"]
                },
                {
                    "word": "Ø³Ù„Ø§Ù…",
                    "root": "Ø³Ù„Ù…",
                    "meaning": "Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø·Ù…Ø£Ù†ÙŠÙ†Ø©",
                    "detailed_meaning": "Ø§Ù„Ø³ÙƒÙŠÙ†Ø© ÙˆØ§Ù„Ø£Ù…Ø§Ù†ØŒ Ø¹Ø¯Ù… Ø§Ù„Ø­Ø±Ø¨ ÙˆØ§Ù„ØµØ±Ø§Ø¹",
                    "examples": ["Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…", "Ø¯Ø§Ø± Ø§Ù„Ø³Ù„Ø§Ù…"]
                }
            ]
        }
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open(source.file_path, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù JSON ØªØ¬Ø±ÙŠØ¨ÙŠ: {source.file_path}")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        return self.load_from_json('custom_json')
    
    def _save_entry_to_engine(self, entry_data: Dict[str, Any], source: LexiconSource):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¯Ø®Ù„ ÙÙŠ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù…."""
        
        try:
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø­Ø±Ùƒ
            lexicon_entry = self.lexicon_engine.analyze_word_revolutionary(
                entry_data['word'], deep_analysis=False
            )
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ
            lexicon_entry.meaning = entry_data.get('meaning', lexicon_entry.meaning)
            lexicon_entry.detailed_meaning = entry_data.get('detailed_meaning', lexicon_entry.detailed_meaning)
            lexicon_entry.usage_examples = entry_data.get('usage_examples', [])
            lexicon_entry.source = source
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø¯Ø®Ù„ '{entry_data['word']}': {e}")
    
    def load_all_available_sources(self) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©."""
        
        print("ğŸ”„ Ø¨Ø¯Ø¡ ØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©...")
        
        results = {}
        
        for source_name, source in self.data_sources.items():
            if not source.is_active:
                continue
            
            print(f"\nğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ù…Ù†: {source.name}")
            
            try:
                if source.source_type == 'api':
                    result = self.load_from_api(source_name)
                elif source.source_type == 'xml':
                    result = self.load_from_xml(source_name)
                elif source.source_type == 'json':
                    result = self.load_from_json(source_name)
                else:
                    result = {'success': False, 'error': f'Ù†ÙˆØ¹ Ù…ØµØ¯Ø± ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {source.source_type}'}
                
                results[source_name] = result
                
                if result['success']:
                    print(f"âœ… Ù†Ø¬Ø­ ØªØ­Ù…ÙŠÙ„ {result['entries_loaded']} Ù…Ø¯Ø®Ù„ Ù…Ù† {source.name}")
                else:
                    print(f"âŒ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ù…Ù† {source.name}: {result['error']}")
                
            except Exception as e:
                results[source_name] = {'success': False, 'error': str(e)}
                print(f"ğŸ’¥ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ {source.name}: {e}")
        
        self.loading_stats['sources_loaded'] = len([r for r in results.values() if r['success']])
        
        print(f"\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù…ÙŠÙ„:")
        print(f"   Ù…ØµØ§Ø¯Ø± Ù…Ø­Ù…Ù„Ø©: {self.loading_stats['sources_loaded']}")
        print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª: {self.loading_stats['total_entries_loaded']}")
        
        return results
    
    def get_loader_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ù…Ø­Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""
        
        return {
            'loader_info': {
                'creation_time': self.creation_time.isoformat(),
                'type': 'lexicon_data_loader'
            },
            'available_sources': {
                name: {
                    'name': source.name,
                    'type': source.source_type,
                    'description': source.description,
                    'is_active': source.is_active
                } for name, source in self.data_sources.items()
            },
            'loading_statistics': self.loading_stats,
            'capabilities': {
                'api_loading': True,
                'xml_loading': True,
                'json_loading': True,
                'text_loading': True,
                'database_loading': True,
                'automatic_sample_creation': True
            }
        }


# === Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹ ===

def quick_load_lexicon_data(engine: ArabicLexiconEngine) -> Dict[str, Any]:
    """ØªØ­Ù…ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¬Ù…."""
    
    loader = LexiconDataLoader(engine)
    results = loader.load_all_available_sources()
    return results


def create_sample_lexicon_files():
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª Ù…Ø¹Ø¬Ù… ØªØ¬Ø±ÙŠØ¨ÙŠØ©."""
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ù…Ø¤Ù‚Øª
    from .arabic_lexicon_engine import ArabicLexiconEngine
    engine = ArabicLexiconEngine("SampleEngine")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ù…Ù„
    loader = LexiconDataLoader(engine)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
    results = {}
    
    # JSON
    json_result = loader.load_from_json('custom_json')
    results['json'] = json_result
    
    # XML
    xml_result = loader.load_from_xml('lisan_xml')
    results['xml'] = xml_result
    
    return results


if __name__ == "__main__":
    print("ğŸ“¥ Ù…Ø­Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø«ÙˆØ±ÙŠ")
    print("ğŸ”— Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„ÙØ§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©...")
    
    results = create_sample_lexicon_files()
    
    for format_type, result in results.items():
        if result['success']:
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù {format_type.upper()}: {result['entries_loaded']} Ù…Ø¯Ø®Ù„")
        else:
            print(f"âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù {format_type.upper()}: {result['error']}")

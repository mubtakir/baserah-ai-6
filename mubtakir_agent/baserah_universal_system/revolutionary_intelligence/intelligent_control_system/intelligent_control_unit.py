#!/usr/bin/env python3
# intelligent_control_unit.py - ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ Baserah Ø§Ù„Ù†Ù‚ÙŠØ©

import numpy as np
import math
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime
import uuid

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ù†ÙˆØ§Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠØ©
from artistic_intelligence.baserah_core import baserah_sigmoid, baserah_linear, baserah_quantum_sigmoid

class BaserahTauBalanceUnit:
    """
    ÙˆØ­Ø¯Ø© ØªÙˆØ§Ø²Ù† Tau Baserah Ø§Ù„Ù†Ù‚ÙŠØ©
    Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† TauRLayer ÙˆÙ„ÙƒÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ ÙˆØ§Ù„Ø®Ø·ÙŠ ÙÙ‚Ø·
    """
    
    def __init__(self, alpha: float = 0.1, beta: float = 0.1):
        """ØªÙ‡ÙŠØ¦Ø© ÙˆØ­Ø¯Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†."""
        
        self.alpha = alpha
        self.beta = beta
        self.epsilon = 1e-6
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙƒÙŠÙ
        self.progress_weight = 1.0
        self.risk_weight = 1.0
        self.adaptation_rate = 0.01
        
        print("âš–ï¸ ØªÙ… ØªÙ‡ÙŠØ¦Ø© ÙˆØ­Ø¯Ø© ØªÙˆØ§Ø²Ù† Tau Baserah Ø§Ù„Ù†Ù‚ÙŠØ©")
    
    def calculate_tau_balance(self, progress: float, risk: float) -> float:
        """
        Ø­Ø³Ø§Ø¨ ØªÙˆØ§Ø²Ù† Tau Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Baserah Ø§Ù„Ù†Ù‚ÙŠØ©.
        
        Args:
            progress: Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„ØªÙ‚Ø¯Ù…
            risk: Ù…Ù‚Ø¯Ø§Ø± Ø§Ù„Ù…Ø®Ø§Ø·Ø±
            
        Returns:
            Ù‚ÙŠÙ…Ø© Ø§Ù„ØªÙˆØ§Ø²Ù† Tau
        """
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø·ÙŠ
        progress_transformed = baserah_linear(
            progress, 
            beta=self.progress_weight, 
            gamma=self.alpha
        )
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø·ÙŠ
        risk_transformed = baserah_linear(
            risk, 
            beta=self.risk_weight, 
            gamma=self.beta
        )
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ø³Ø¨Ø©
        denominator = risk_transformed + self.epsilon
        ratio = progress_transformed / denominator
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…ØªÙˆØ§Ø²Ù†Ø©
        tau_balance = baserah_sigmoid(
            ratio, 
            n=1, 
            k=2.0, 
            x0=0.0, 
            alpha=1.0
        )
        
        return tau_balance
    
    def adapt_weights(self, feedback: float):
        """ØªÙƒÙŠÙŠÙ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©."""
        
        if feedback > 0:  # ØªØ­Ø³Ù†
            self.progress_weight += self.adaptation_rate
            self.risk_weight *= 0.99
        else:  # ØªØ±Ø§Ø¬Ø¹
            self.progress_weight *= 0.99
            self.risk_weight += self.adaptation_rate
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯
        self.progress_weight = max(0.1, min(2.0, self.progress_weight))
        self.risk_weight = max(0.1, min(2.0, self.risk_weight))

class BaserahDynamicMathUnit:
    """
    ÙˆØ­Ø¯Ø© Ø±ÙŠØ§Ø¶ÙŠØ© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Baserah Ø§Ù„Ù†Ù‚ÙŠØ©
    Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† DynamicMathUnit ÙˆÙ„ÙƒÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯ ÙˆØ§Ù„Ø®Ø·ÙŠ ÙÙ‚Ø·
    """
    
    def __init__(self, complexity: int = 5):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©."""
        
        self.complexity = complexity
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
        self.sigmoid_coeffs = np.random.randn(complexity) * 0.1
        self.sigmoid_k_values = np.random.rand(complexity) * 2.0 + 0.5
        self.sigmoid_x0_values = np.random.randn(complexity) * 0.5
        
        # Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙŠ
        self.linear_betas = np.random.randn(complexity) * 0.1
        self.linear_gammas = np.random.randn(complexity) * 0.1
        
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªØ±ÙƒÙŠØ¨
        self.composition_weights = np.random.rand(complexity)
        self.composition_weights /= np.sum(self.composition_weights)
        
        print(f"ğŸ§® ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Baserah (ØªØ¹Ù‚ÙŠØ¯: {complexity})")
    
    def compute_dynamic_output(self, x: float) -> float:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Baserah Ø§Ù„Ù†Ù‚ÙŠØ©.
        
        Args:
            x: Ø§Ù„Ù…Ø¯Ø®Ù„
            
        Returns:
            Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ
        """
        
        total_output = 0.0
        
        for i in range(self.complexity):
            # Ù…ÙƒÙˆÙ† Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
            sigmoid_component = baserah_sigmoid(
                x,
                n=1,
                k=self.sigmoid_k_values[i],
                x0=self.sigmoid_x0_values[i],
                alpha=1.0
            )
            
            # Ù…ÙƒÙˆÙ† Ø§Ù„Ø®Ø·ÙŠ
            linear_component = baserah_linear(
                x,
                beta=self.linear_betas[i],
                gamma=self.linear_gammas[i]
            )
            
            # ØªØ±ÙƒÙŠØ¨ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
            combined_component = (
                self.sigmoid_coeffs[i] * sigmoid_component +
                (1 - self.sigmoid_coeffs[i]) * linear_component
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
            total_output += self.composition_weights[i] * combined_component
        
        return total_output
    
    def evolve_parameters(self, feedback: float):
        """ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©."""
        
        evolution_rate = 0.01
        
        if feedback > 0:  # ØªØ­Ø³Ù†
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            self.sigmoid_k_values *= (1 + evolution_rate)
            self.linear_betas *= (1 + evolution_rate)
        else:  # ØªØ±Ø§Ø¬Ø¹
            # Ø¥Ø¶Ø§ÙØ© ØªÙ†ÙˆÙŠØ¹
            noise_sigmoid = np.random.randn(self.complexity) * evolution_rate
            noise_linear = np.random.randn(self.complexity) * evolution_rate
            
            self.sigmoid_k_values += noise_sigmoid
            self.linear_betas += noise_linear
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯
        self.sigmoid_k_values = np.clip(self.sigmoid_k_values, 0.1, 5.0)
        self.linear_betas = np.clip(self.linear_betas, -2.0, 2.0)

class BaserahIntelligentControlUnit:
    """
    ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ Baserah Ø§Ù„Ù†Ù‚ÙŠØ©
    Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† IMRLS ÙˆÙ„ÙƒÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†Ù‡Ø¬Ù†Ø§ Ø§Ù„Ø«ÙˆØ±ÙŠ ÙÙ‚Ø·
    """
    
    def __init__(self, input_dim: int = 3, output_dim: int = 3, complexity: int = 5):
        """ØªÙ‡ÙŠØ¦Ø© ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ."""
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.complexity = complexity
        
        # Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.tau_balance_unit = BaserahTauBalanceUnit()
        self.dynamic_math_units = [
            BaserahDynamicMathUnit(complexity) for _ in range(output_dim)
        ]
        
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„ØªØ¬Ø§Ø±Ø¨
        self.experience_memory = []
        self.max_memory_size = 1000
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_history = []
        self.adaptation_count = 0
        
        print(f"ğŸ§  ØªÙ… ØªÙ‡ÙŠØ¦Ø© ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ Baserah Ø§Ù„Ù†Ù‚ÙŠØ©")
        print(f"   ğŸ“Š Ø£Ø¨Ø¹Ø§Ø¯: {input_dim} â†’ {output_dim}")
        print(f"   ğŸ”§ ØªØ¹Ù‚ÙŠØ¯: {complexity}")
    
    def process_input(self, inputs: List[float], target: List[float] = None) -> List[float]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ¥Ù†ØªØ§Ø¬ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ØªØ­ÙƒÙ….
        
        Args:
            inputs: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            target: Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            
        Returns:
            Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ØªØ­ÙƒÙ…
        """
        
        if len(inputs) != self.input_dim:
            raise ValueError(f"Expected {self.input_dim} inputs, got {len(inputs)}")
        
        outputs = []
        
        for i in range(self.output_dim):
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¬ Ù„ÙƒÙ„ Ø¨Ø¹Ø¯
            output_value = 0.0
            
            for j, input_val in enumerate(inputs):
                # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù…Ø¯Ø®Ù„ Ø¨Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
                processed_input = self.dynamic_math_units[i].compute_dynamic_output(input_val)
                
                # ØªØ·Ø¨ÙŠÙ‚ ÙˆØ²Ù† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹
                weight = baserah_sigmoid(
                    j - i, 
                    n=1, 
                    k=1.0, 
                    x0=0.0, 
                    alpha=1.0
                )
                
                output_value += weight * processed_input
            
            # ØªØ·Ø¨ÙŠÙ‚ ØªÙˆØ§Ø²Ù† Tau Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù‡Ø¯Ù Ù…ØªØ§Ø­Ø§Ù‹
            if target and i < len(target):
                error = abs(target[i] - output_value)
                progress = max(0, 1.0 - error)
                risk = error
                
                tau_balance = self.tau_balance_unit.calculate_tau_balance(progress, risk)
                output_value *= tau_balance
            
            outputs.append(output_value)
        
        return outputs
    
    def learn_from_experience(self, inputs: List[float], outputs: List[float], 
                            reward: float, target: List[float] = None):
        """
        Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª.
        
        Args:
            inputs: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            outputs: Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
            reward: Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
            target: Ø§Ù„Ù‡Ø¯Ù (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        """
        
        # Ø­ÙØ¸ Ø§Ù„ØªØ¬Ø±Ø¨Ø©
        experience = {
            'inputs': inputs.copy(),
            'outputs': outputs.copy(),
            'reward': reward,
            'target': target.copy() if target else None,
            'timestamp': datetime.now()
        }
        
        self.experience_memory.append(experience)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if len(self.experience_memory) > self.max_memory_size:
            self.experience_memory.pop(0)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£Ø¯Ø§Ø¡
        self.performance_history.append(reward)
        
        # Ø§Ù„ØªÙƒÙŠÙ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©
        self._adapt_parameters(reward)
        
        self.adaptation_count += 1
    
    def _adapt_parameters(self, reward: float):
        """ØªÙƒÙŠÙŠÙ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©."""
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© ØªØ­Ø³Ù† Ø£Ù… Ù„Ø§
        improvement = 0.0
        if len(self.performance_history) > 1:
            improvement = reward - self.performance_history[-2]
        
        # ØªÙƒÙŠÙŠÙ ÙˆØ­Ø¯Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†
        self.tau_balance_unit.adapt_weights(improvement)
        
        # ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
        for unit in self.dynamic_math_units:
            unit.evolve_parameters(improvement)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡."""
        
        if not self.performance_history:
            return {'message': 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¯Ø§Ø¡'}
        
        recent_performance = self.performance_history[-10:] if len(self.performance_history) >= 10 else self.performance_history
        
        summary = {
            'total_experiences': len(self.experience_memory),
            'adaptation_count': self.adaptation_count,
            'average_reward': np.mean(self.performance_history),
            'recent_average_reward': np.mean(recent_performance),
            'best_reward': max(self.performance_history),
            'worst_reward': min(self.performance_history),
            'improvement_trend': self._calculate_trend(),
            'tau_balance_weights': {
                'progress_weight': self.tau_balance_unit.progress_weight,
                'risk_weight': self.tau_balance_unit.risk_weight
            }
        }
        
        return summary
    
    def _calculate_trend(self) -> str:
        """Ø­Ø³Ø§Ø¨ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ­Ø³Ù†."""
        
        if len(self.performance_history) < 5:
            return 'ØºÙŠØ± ÙƒØ§ÙÙŠ Ù„Ù„ØªÙ‚ÙŠÙŠÙ…'
        
        recent = self.performance_history[-5:]
        earlier = self.performance_history[-10:-5] if len(self.performance_history) >= 10 else self.performance_history[:-5]
        
        if not earlier:
            return 'ØºÙŠØ± ÙƒØ§ÙÙŠ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©'
        
        recent_avg = np.mean(recent)
        earlier_avg = np.mean(earlier)
        
        if recent_avg > earlier_avg + 0.1:
            return 'ØªØ­Ø³Ù† Ù…Ø³ØªÙ…Ø±'
        elif recent_avg < earlier_avg - 0.1:
            return 'ØªØ±Ø§Ø¬Ø¹'
        else:
            return 'Ù…Ø³ØªÙ‚Ø±'
    
    def demonstrate_control(self) -> Dict[str, Any]:
        """Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ…."""
        
        print("\nğŸ­ Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ù„ÙˆØ­Ø¯Ø© Ø§Ù„ØªØ­ÙƒÙ… Ø§Ù„Ø°ÙƒÙŠ Baserah")
        print("=" * 50)
        
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
        test_scenarios = [
            {
                'name': 'ØªØ­ÙƒÙ… Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ·',
                'inputs': [1.0, 0.5, -0.2],
                'target': [2.0, 1.0, 0.0]
            },
            {
                'name': 'ØªØ­ÙƒÙ… Ù…Ø¹Ù‚Ø¯',
                'inputs': [-1.5, 2.0, 0.8],
                'target': [0.0, 0.0, 1.0]
            },
            {
                'name': 'ØªØ­ÙƒÙ… Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ',
                'inputs': [0.3, -0.7, 1.2],
                'target': [-1.0, 2.0, -0.5]
            }
        ]
        
        results = []
        
        for scenario in test_scenarios:
            print(f"\nğŸ”§ Ø§Ø®ØªØ¨Ø§Ø±: {scenario['name']}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
            outputs = self.process_input(scenario['inputs'], scenario['target'])
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£
            if scenario['target']:
                errors = [abs(t - o) for t, o in zip(scenario['target'], outputs)]
                total_error = sum(errors)
                reward = max(0, 1.0 - total_error)
            else:
                reward = 0.5
            
            # Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØ¬Ø±Ø¨Ø©
            self.learn_from_experience(
                scenario['inputs'], 
                outputs, 
                reward, 
                scenario['target']
            )
            
            result = {
                'scenario': scenario['name'],
                'inputs': scenario['inputs'],
                'outputs': outputs,
                'target': scenario['target'],
                'reward': reward,
                'errors': errors if scenario['target'] else None
            }
            
            results.append(result)
            
            print(f"   ğŸ“Š Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª: {scenario['inputs']}")
            print(f"   ğŸ“ˆ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª: {[f'{o:.3f}' for o in outputs]}")
            print(f"   ğŸ¯ Ø§Ù„Ù‡Ø¯Ù: {scenario['target']}")
            print(f"   ğŸ† Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {reward:.3f}")
        
        # Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡
        performance = self.get_performance_summary()
        print(f"\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø£Ø¯Ø§Ø¡:")
        print(f"   Ø§Ù„ØªØ¬Ø§Ø±Ø¨: {performance['total_experiences']}")
        print(f"   Ø§Ù„ØªÙƒÙŠÙØ§Øª: {performance['adaptation_count']}")
        print(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©: {performance['average_reward']:.3f}")
        print(f"   Ø§ØªØ¬Ø§Ù‡ Ø§Ù„ØªØ­Ø³Ù†: {performance['improvement_trend']}")
        
        return {
            'test_results': results,
            'performance_summary': performance
        }

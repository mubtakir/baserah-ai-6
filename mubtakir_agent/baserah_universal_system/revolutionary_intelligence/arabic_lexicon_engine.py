#!/usr/bin/env python3
# arabic_lexicon_engine.py - Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ
#
# ğŸ‘¨â€ğŸ’» Ø§Ù„Ù…Ø·ÙˆØ±: Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
# ğŸ§  Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±ÙŠØ©: Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙˆØ§Ù„Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù…Ù† Ø¥Ø¨Ø¯Ø§Ø¹ Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
# ğŸ¤– Ø§Ù„ØªØ·ÙˆÙŠØ±: Ø£ÙƒÙˆØ§Ø¯ Ø¨Ø¯Ø§Ø¦ÙŠØ© ØªÙ… ØªØ·ÙˆÙŠØ±Ù‡Ø§ Ø¨Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆÙƒÙŠÙ„ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
# ğŸ“… ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡: 2025
# ğŸ“š Ø§Ù„Ù…Ø­Ø±Ùƒ: Ù…Ø­Ø±Ùƒ Ø«ÙˆØ±ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
# ğŸŒŸ Ø§Ù„Ù†Ø¸Ø§Ù…: Baserah Pure System - sigmoid + linear ÙÙ‚Ø·ØŒ Ø¨Ø¯ÙˆÙ† AI ØªÙ‚Ù„ÙŠØ¯ÙŠ
# Ù…Ø¯Ù…Ø¬ Ù…Ø¹ Ù†Ø¸Ø§Ù… Baserah ÙˆÙ†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ ÙˆØ§Ù„ÙƒÙ„Ù…Ø§Øª

import os
import sys
import json
import sqlite3
import requests
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø«ÙˆØ±ÙŠ
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†ÙˆØ§Ø© Ø§Ù„ÙÙ†ÙŠØ©
from artistic_intelligence.baserah_core import (
    baserah_sigmoid, baserah_linear, baserah_quantum_sigmoid
)

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
from .ai_oop_foundation import BaserahExpertExplorerFoundation
from .revolutionary_mother_equation import ConcreteRevolutionaryMotherEquation


class LexiconSource(Enum):
    """Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."""
    LISAN_AL_ARAB = "lisan_al_arab"
    TAJ_AL_AROUS = "taj_al_arous"
    QAMOUS_MUHIT = "qamous_muhit"
    MUKHTAR_SAHAH = "mukhtar_sahah"
    WASEET = "waseet"
    AL_GHANI = "al_ghani"
    CUSTOM_DATABASE = "custom_database"


class LetterMeaning(Enum):
    """Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©."""
    ALIF = "Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ø£Ù„ÙˆÙ‡ÙŠØ©"
    BAA = "Ø§Ù„Ø¨ÙŠØª ÙˆØ§Ù„Ø§Ø­ØªÙˆØ§Ø¡ ÙˆØ§Ù„Ø¨Ø±ÙƒØ©"
    TAA = "Ø§Ù„ØªØ§Ø¬ ÙˆØ§Ù„Ø¹Ù„Ùˆ ÙˆØ§Ù„ØªÙ…Ø§Ù…"
    THAA = "Ø§Ù„Ø«Ø¨Ø§Øª ÙˆØ§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±"
    JEEM = "Ø§Ù„Ø¬Ù…Ø¹ ÙˆØ§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ ÙˆØ§Ù„Ø¬Ù…Ø§Ù„"
    HAA_HOTI = "Ø§Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ù„Ø­ÙŠÙˆÙŠØ©"
    KHAA = "Ø§Ù„Ø®Ø±ÙˆØ¬ ÙˆØ§Ù„Ø®Ù„Ø§Øµ"
    DAL = "Ø§Ù„Ø¯Ù„Ø§Ù„Ø© ÙˆØ§Ù„Ø¥Ø±Ø´Ø§Ø¯"
    THAL = "Ø§Ù„Ø°Ù„ ÙˆØ§Ù„Ø®Ø¶ÙˆØ¹"
    RAA = "Ø§Ù„Ø±Ø­Ù…Ø© ÙˆØ§Ù„Ø±Ù‚Ø©"
    ZAIN = "Ø§Ù„Ø²ÙŠÙ†Ø© ÙˆØ§Ù„Ø¬Ù…Ø§Ù„"
    SEEN = "Ø§Ù„Ø³Ø±Ø¹Ø© ÙˆØ§Ù„Ø³ÙŠØ±"
    SHEEN = "Ø§Ù„Ø´Ø¹Ø§Ø¹ ÙˆØ§Ù„Ø§Ù†ØªØ´Ø§Ø±"
    SAD = "Ø§Ù„ØµØ¯Ù‚ ÙˆØ§Ù„ØµÙ„Ø§Ø¨Ø©"
    DAD = "Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„Ù‚ÙˆØ©"
    TAA_MUTBAQA = "Ø§Ù„Ø·Ù‡Ø§Ø±Ø© ÙˆØ§Ù„Ù†Ø¸Ø§ÙØ©"
    DHAA = "Ø§Ù„Ø¸Ù‡ÙˆØ± ÙˆØ§Ù„ÙˆØ¶ÙˆØ­"
    AIN = "Ø§Ù„Ø¹ÙŠÙ† ÙˆØ§Ù„Ø±Ø¤ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ù…"
    GHAIN = "Ø§Ù„ØºÙ…ÙˆØ¶ ÙˆØ§Ù„Ø®ÙØ§Ø¡"
    FAA = "Ø§Ù„ÙØªØ­ ÙˆØ§Ù„Ø§Ù†ÙØªØ§Ø­"
    QAF = "Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø´Ø¯Ø©"
    KAF = "Ø§Ù„ÙƒÙ ÙˆØ§Ù„Ø¥Ù…Ø³Ø§Ùƒ"
    LAM = "Ø§Ù„Ù„Ø³Ø§Ù† ÙˆØ§Ù„ÙƒÙ„Ø§Ù…"
    MEEM = "Ø§Ù„Ù…Ø§Ø¡ ÙˆØ§Ù„Ø­ÙŠØ§Ø©"
    NOON = "Ø§Ù„Ù†ÙˆØ± ÙˆØ§Ù„Ø¥Ø¶Ø§Ø¡Ø©"
    HAA = "Ø§Ù„Ù‡ÙˆØ§Ø¡ ÙˆØ§Ù„ØªÙ†ÙØ³"
    WAW = "Ø§Ù„ÙˆØ§Ùˆ ÙˆØ§Ù„Ø±Ø¨Ø·"
    YAA = "Ø§Ù„ÙŠØ¯ ÙˆØ§Ù„Ø¹Ù…Ù„"


@dataclass
class LexiconEntry:
    """Ù…Ø¯Ø®Ù„ Ù…Ø¹Ø¬Ù…ÙŠ Ø«ÙˆØ±ÙŠ."""
    word: str
    root: str
    meaning: str
    detailed_meaning: str
    source: LexiconSource
    letter_analysis: Dict[str, str]
    baserah_analysis: Dict[str, float]
    basil_theories_application: Dict[str, Any]
    semantic_weight: float
    usage_examples: List[str]
    related_words: List[str]
    metadata: Dict[str, Any]


@dataclass
class LetterAnalysis:
    """ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ Ù„Ù„Ø­Ø±ÙˆÙ."""
    letter: str
    position: int
    meaning: str
    baserah_value: float
    basil_theory_applied: str
    semantic_contribution: float
    revolutionary_insights: List[str]


class ArabicLexiconEngine(BaserahExpertExplorerFoundation):
    """
    Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ.
    
    ÙŠØ¯Ù…Ø¬:
    1. Ù…Ø¹Ø§Ø¬Ù… Ø¹Ø±Ø¨ÙŠØ© Ù…ØªØ¹Ø¯Ø¯Ø© (Ù„Ø³Ø§Ù† Ø§Ù„Ø¹Ø±Ø¨ØŒ ØªØ§Ø¬ Ø§Ù„Ø¹Ø±ÙˆØ³ØŒ Ø¥Ù„Ø®)
    2. ØªØ­Ù„ÙŠÙ„ Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ Ø¨Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„
    3. Ù…Ø¹Ø§Ù„Ø¬Ø© Baserah Ø§Ù„Ù†Ù‚ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù†ÙŠ
    4. Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ø§Ø´ØªÙ‚Ø§Ù‚Ø§Øª
    5. Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ø§Ù„Ø¨Ø§Ù‡Ø±
    """
    
    def __init__(self, engine_name: str = "ArabicLexiconEngine",
                 mother_inheritance: ConcreteRevolutionaryMotherEquation = None):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ."""
        
        # Ø§Ù„ÙˆØ±Ø§Ø«Ø© Ù…Ù† Ø§Ù„Ø£Ø³Ø³ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        super().__init__(engine_name, mother_inheritance)
        
        # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ
        self.engine_name = engine_name
        self.creation_time = datetime.now()
        self.version = "1.0.0 - Arabic Lexicon Engine"
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¬Ù…
        self.lexicon_db_path = "data/arabic_lexicon.db"
        self.letter_meanings_path = "ref/Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø­Ø±ÙˆÙ.txt"
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ
        self.engine_stats = {
            'words_analyzed': 0,
            'letters_analyzed': 0,
            'roots_discovered': 0,
            'meanings_extracted': 0,
            'baserah_analyses_performed': 0,
            'basil_theories_applications': 0,
            'average_semantic_weight': 0.0,
            'total_revolutionary_insights': 0
        }
        
        # Ù…Ø¹Ø§Ø¬Ù… Ù…Ø¯Ù…Ø¬Ø©
        self.integrated_lexicons = {
            LexiconSource.LISAN_AL_ARAB: {},
            LexiconSource.CUSTOM_DATABASE: {}
        }
        
        # Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ
        self.letter_meanings = self._load_letter_meanings()
        
        # ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        self._initialize_database()
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self._load_basic_lexicons()
        
        print(f"ğŸ“š ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ: {engine_name}")
        print(f"ğŸ”¤ Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ: {len(self.letter_meanings)} Ø­Ø±Ù")
        print(f"ğŸ“– Ù…Ø¹Ø§Ø¬Ù… Ù…Ø¯Ù…Ø¬Ø©: {len(self.integrated_lexicons)}")
        print(f"ğŸ§¬ Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠØ©")
    
    def _load_letter_meanings(self) -> Dict[str, str]:
        """ØªØ­Ù…ÙŠÙ„ Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."""
        
        letter_meanings = {}
        
        # Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        arabic_letters = {
            'Ø§': LetterMeaning.ALIF.value,
            'Ø¨': LetterMeaning.BAA.value,
            'Øª': LetterMeaning.TAA.value,
            'Ø«': LetterMeaning.THAA.value,
            'Ø¬': LetterMeaning.JEEM.value,
            'Ø­': LetterMeaning.HAA_HOTI.value,
            'Ø®': LetterMeaning.KHAA.value,
            'Ø¯': LetterMeaning.DAL.value,
            'Ø°': LetterMeaning.THAL.value,
            'Ø±': LetterMeaning.RAA.value,
            'Ø²': LetterMeaning.ZAIN.value,
            'Ø³': LetterMeaning.SEEN.value,
            'Ø´': LetterMeaning.SHEEN.value,
            'Øµ': LetterMeaning.SAD.value,
            'Ø¶': LetterMeaning.DAD.value,
            'Ø·': LetterMeaning.TAA_MUTBAQA.value,
            'Ø¸': LetterMeaning.DHAA.value,
            'Ø¹': LetterMeaning.AIN.value,
            'Øº': LetterMeaning.GHAIN.value,
            'Ù': LetterMeaning.FAA.value,
            'Ù‚': LetterMeaning.QAF.value,
            'Ùƒ': LetterMeaning.KAF.value,
            'Ù„': LetterMeaning.LAM.value,
            'Ù…': LetterMeaning.MEEM.value,
            'Ù†': LetterMeaning.NOON.value,
            'Ù‡': LetterMeaning.HAA.value,
            'Ùˆ': LetterMeaning.WAW.value,
            'ÙŠ': LetterMeaning.YAA.value
        }
        
        letter_meanings.update(arabic_letters)
        
        # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ù…Ù„Ù Ø¥Ø¶Ø§ÙÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        try:
            if os.path.exists(self.letter_meanings_path):
                with open(self.letter_meanings_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù†ÙŠ Ø¥Ø¶Ø§ÙÙŠØ©
                    lines = content.split('\n')
                    for line in lines:
                        if ':' in line:
                            parts = line.split(':', 1)
                            if len(parts) == 2:
                                letter = parts[0].strip()
                                meaning = parts[1].strip()
                                if letter in letter_meanings:
                                    letter_meanings[letter] += f" | {meaning}"
                                else:
                                    letter_meanings[letter] = meaning
        except Exception as e:
            print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ù„Ù Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„Ø­Ø±ÙˆÙ: {e}")
        
        return letter_meanings
    
    def _initialize_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¬Ù…."""
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        os.makedirs(os.path.dirname(self.lexicon_db_path), exist_ok=True)
        
        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS lexicon_entries (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    word TEXT NOT NULL,
                    root TEXT,
                    meaning TEXT,
                    detailed_meaning TEXT,
                    source TEXT,
                    letter_analysis TEXT,
                    baserah_analysis TEXT,
                    basil_theories TEXT,
                    semantic_weight REAL,
                    usage_examples TEXT,
                    related_words TEXT,
                    metadata TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS letter_analyses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    word TEXT NOT NULL,
                    letter TEXT NOT NULL,
                    position INTEGER,
                    meaning TEXT,
                    baserah_value REAL,
                    basil_theory TEXT,
                    semantic_contribution REAL,
                    revolutionary_insights TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¬Ø°ÙˆØ±
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS roots (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    root TEXT UNIQUE NOT NULL,
                    meaning TEXT,
                    derived_words TEXT,
                    baserah_analysis TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # ÙÙ‡Ø§Ø±Ø³ Ù„Ù„Ø¨Ø­Ø« Ø§Ù„Ø³Ø±ÙŠØ¹
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_word ON lexicon_entries(word)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_root ON lexicon_entries(root)')
            cursor.execute('CREATE INDEX IF NOT EXISTS idx_letter ON letter_analyses(letter)')
            
            conn.commit()
            conn.close()
            
            print(f"âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø¬Ù…: {self.lexicon_db_path}")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
    
    def _load_basic_lexicons(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©."""
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø¬Ù… Ø£Ø³Ø§Ø³ÙŠ Ù…Ø¯Ù…Ø¬
        basic_lexicon = {
            # ÙƒÙ„Ù…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ
            'Ø§Ù„Ù„Ù‡': {
                'root': 'Ø£Ù„Ù‡',
                'meaning': 'Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯',
                'detailed_meaning': 'Ø§Ø³Ù… Ø§Ù„Ø¬Ù„Ø§Ù„Ø©ØŒ Ø§Ù„Ø¥Ù„Ù‡ Ø§Ù„Ù…Ø¹Ø¨ÙˆØ¯ Ø¨Ø­Ù‚ØŒ Ø§Ù„ÙˆØ§Ø­Ø¯ Ø§Ù„Ø£Ø­Ø¯ Ø§Ù„ÙØ±Ø¯ Ø§Ù„ØµÙ…Ø¯',
                'letter_analysis': {
                    'Ø§': 'Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„Ø¨Ø¯Ø§ÙŠØ©',
                    'Ù„': 'Ø§Ù„Ù„Ø³Ø§Ù† ÙˆØ§Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø¥Ù„Ù‡ÙŠ',
                    'Ù„': 'Ø§Ù„ØªØ£ÙƒÙŠØ¯ ÙˆØ§Ù„ØªÙƒØ±Ø§Ø±',
                    'Ù‡': 'Ø§Ù„Ù‡ÙˆØ§Ø¡ ÙˆØ§Ù„Ø±ÙˆØ­ Ø§Ù„Ø¥Ù„Ù‡ÙŠØ©'
                }
            },
            'Ø­ÙŠØ§Ø©': {
                'root': 'Ø­ÙŠÙŠ',
                'meaning': 'Ø§Ù„ÙˆØ¬ÙˆØ¯ ÙˆØ§Ù„Ø¨Ù‚Ø§Ø¡',
                'detailed_meaning': 'Ø§Ù„Ù†Ù…Ùˆ ÙˆØ§Ù„Ø­Ø±ÙƒØ© ÙˆØ§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø± ÙÙŠ Ø§Ù„ÙˆØ¬ÙˆØ¯',
                'letter_analysis': {
                    'Ø­': 'Ø§Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ù„Ø­ÙŠÙˆÙŠØ©',
                    'ÙŠ': 'Ø§Ù„ÙŠØ¯ ÙˆØ§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø­Ø±ÙƒØ©',
                    'Ø§': 'Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„Ø§Ø³ØªÙ…Ø±Ø§Ø±',
                    'Ø©': 'Ø§Ù„ØªØ£Ù†ÙŠØ« ÙˆØ§Ù„Ø±Ù‚Ø©'
                }
            },
            'Ø¹Ù„Ù…': {
                'root': 'Ø¹Ù„Ù…',
                'meaning': 'Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ',
                'detailed_meaning': 'Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ø´ÙŠØ¡ Ø¹Ù„Ù‰ Ù…Ø§ Ù‡Ùˆ Ø¹Ù„ÙŠÙ‡ Ø¥Ø¯Ø±Ø§ÙƒØ§Ù‹ Ø¬Ø§Ø²Ù…Ø§Ù‹',
                'letter_analysis': {
                    'Ø¹': 'Ø§Ù„Ø¹ÙŠÙ† ÙˆØ§Ù„Ø±Ø¤ÙŠØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ',
                    'Ù„': 'Ø§Ù„Ù„Ø³Ø§Ù† ÙˆØ§Ù„ØªØ¹Ø¨ÙŠØ±',
                    'Ù…': 'Ø§Ù„Ù…Ø§Ø¡ ÙˆØ§Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ù„Ù†Ù…Ø§Ø¡'
                }
            },
            'Ø­ÙƒÙ…Ø©': {
                'root': 'Ø­ÙƒÙ…',
                'meaning': 'Ø§Ù„Ø¹Ø¯Ù„ ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù†',
                'detailed_meaning': 'ÙˆØ¶Ø¹ Ø§Ù„Ø´ÙŠØ¡ ÙÙŠ Ù…ÙˆØ¶Ø¹Ù‡ Ø§Ù„ØµØ­ÙŠØ­ØŒ Ø§Ù„Ø¹Ø¯Ù„ ÙˆØ§Ù„Ø¥ØªÙ‚Ø§Ù†',
                'letter_analysis': {
                    'Ø­': 'Ø§Ù„Ø­ÙŠØ§Ø© ÙˆØ§Ù„Ø­ÙŠÙˆÙŠØ©',
                    'Ùƒ': 'Ø§Ù„ÙƒÙ ÙˆØ§Ù„Ø¥Ù…Ø³Ø§Ùƒ ÙˆØ§Ù„Ø¶Ø¨Ø·',
                    'Ù…': 'Ø§Ù„Ù…Ø§Ø¡ ÙˆØ§Ù„Ø­ÙŠØ§Ø©',
                    'Ø©': 'Ø§Ù„ØªØ£Ù†ÙŠØ« ÙˆØ§Ù„Ø±Ù‚Ø©'
                }
            }
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…Ø¯Ù…Ø¬
        self.integrated_lexicons[LexiconSource.CUSTOM_DATABASE] = basic_lexicon
        
        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        for word, data in basic_lexicon.items():
            self._save_lexicon_entry(word, data, LexiconSource.CUSTOM_DATABASE)
        
        print(f"ğŸ“š ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {len(basic_lexicon)} ÙƒÙ„Ù…Ø©")
    
    def analyze_word_revolutionary(self, word: str, deep_analysis: bool = True) -> LexiconEntry:
        """
        ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø©.
        
        Args:
            word: Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡Ø§
            deep_analysis: ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ ÙŠØ´Ù…Ù„ Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„
            
        Returns:
            LexiconEntry: ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„ÙƒÙ„Ù…Ø©
        """
        
        print(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}")
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©
        lexicon_data = self._search_in_lexicons(word)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
        letter_analyses = self._analyze_letters_revolutionary(word)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        root = self._extract_root(word)
        
        # ØªØ­Ù„ÙŠÙ„ Baserah
        baserah_analysis = self._apply_baserah_analysis(word, letter_analyses)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„
        basil_theories = self._apply_basil_theories_to_word(word, letter_analyses) if deep_analysis else {}
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        semantic_weight = self._calculate_semantic_weight(word, letter_analyses, baserah_analysis)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¯Ø®Ù„ Ù…Ø¹Ø¬Ù…ÙŠ Ø«ÙˆØ±ÙŠ
        lexicon_entry = LexiconEntry(
            word=word,
            root=root,
            meaning=lexicon_data.get('meaning', 'Ù…Ø¹Ù†Ù‰ ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
            detailed_meaning=lexicon_data.get('detailed_meaning', ''),
            source=LexiconSource.CUSTOM_DATABASE,
            letter_analysis={letter.letter: letter.meaning for letter in letter_analyses},
            baserah_analysis=baserah_analysis,
            basil_theories_application=basil_theories,
            semantic_weight=semantic_weight,
            usage_examples=lexicon_data.get('usage_examples', []),
            related_words=self._find_related_words(word, root),
            metadata={
                'analysis_time': datetime.now().isoformat(),
                'deep_analysis': deep_analysis,
                'revolutionary_method': True
            }
        )
        
        # Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self._save_analysis_to_database(lexicon_entry, letter_analyses)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self._update_engine_statistics(lexicon_entry, letter_analyses)
        
        print(f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}")
        
        return lexicon_entry

    def _search_in_lexicons(self, word: str) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¬Ù… Ø§Ù„Ù…Ø¯Ù…Ø¬Ø©."""

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…Ø®ØµØµ Ø£ÙˆÙ„Ø§Ù‹
        if word in self.integrated_lexicons[LexiconSource.CUSTOM_DATABASE]:
            return self.integrated_lexicons[LexiconSource.CUSTOM_DATABASE][word]

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            cursor.execute('SELECT * FROM lexicon_entries WHERE word = ?', (word,))
            result = cursor.fetchone()

            if result:
                return {
                    'meaning': result[3],
                    'detailed_meaning': result[4],
                    'usage_examples': json.loads(result[10]) if result[10] else []
                }

            conn.close()

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")

        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ØŒ Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ
        return {
            'meaning': f'ÙƒÙ„Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©: {word}',
            'detailed_meaning': f'ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©: {word}',
            'usage_examples': []
        }

    def _analyze_letters_revolutionary(self, word: str) -> List[LetterAnalysis]:
        """ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ Ù„Ù„Ø­Ø±ÙˆÙ."""

        letter_analyses = []

        for i, letter in enumerate(word):
            if letter in self.letter_meanings:
                # ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙˆØ§Ù„ Baserah Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±Ù
                letter_value = ord(letter) / 1000.0  # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ø±Ù‚Ù…ÙŠØ©
                baserah_value = baserah_sigmoid(letter_value, n=1, k=1.5, x0=0.5, alpha=1.0)

                # ØªØ­Ø¯ÙŠØ¯ Ù†Ø¸Ø±ÙŠØ© Ø¨Ø§Ø³Ù„ Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©
                basil_theory = self._determine_basil_theory_for_letter(letter, i, len(word))

                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
                semantic_contribution = self._calculate_letter_semantic_contribution(letter, i, word)

                # Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
                revolutionary_insights = self._generate_letter_insights(letter, i, word)

                letter_analysis = LetterAnalysis(
                    letter=letter,
                    position=i,
                    meaning=self.letter_meanings[letter],
                    baserah_value=baserah_value,
                    basil_theory_applied=basil_theory,
                    semantic_contribution=semantic_contribution,
                    revolutionary_insights=revolutionary_insights
                )

                letter_analyses.append(letter_analysis)

        return letter_analyses

    def _determine_basil_theory_for_letter(self, letter: str, position: int, word_length: int) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†Ø¸Ø±ÙŠØ© Ø¨Ø§Ø³Ù„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø­Ø±Ù."""

        # Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± Ù„Ù„Ø­Ø±ÙˆÙ ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ø²ÙˆØ¬ÙŠØ©/Ø§Ù„ÙØ±Ø¯ÙŠØ©
        if position % 2 == 0:
            return "Zero Duality Theory - Ø§Ù„Ù…ÙˆØ¶Ø¹ Ø§Ù„Ø²ÙˆØ¬ÙŠ (Ø¥ÙŠØ¬Ø§Ø¨ÙŠ)"
        else:
            return "Zero Duality Theory - Ø§Ù„Ù…ÙˆØ¶Ø¹ Ø§Ù„ÙØ±Ø¯ÙŠ (Ø³Ø§Ù„Ø¨)"

        # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù…Ù†Ø·Ù‚ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹ Ù‡Ù†Ø§

    def _calculate_letter_semantic_contribution(self, letter: str, position: int, word: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ© Ù„Ù„Ø­Ø±Ù."""

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø©
        position_weight = 1.0 - (position / len(word))  # Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£ÙˆÙ„Ù‰ Ø£Ù‡Ù…
        letter_frequency = word.count(letter) / len(word)  # ØªÙƒØ±Ø§Ø± Ø§Ù„Ø­Ø±Ù

        # ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ø³ÙŠØ¬Ù…ÙˆÙŠØ¯
        contribution = baserah_sigmoid(position_weight * letter_frequency, n=1, k=2.0, x0=0.3, alpha=1.0)

        return contribution

    def _generate_letter_insights(self, letter: str, position: int, word: str) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ø­Ø±Ù."""

        insights = []

        # Ø±Ø¤Ù‰ Ø­Ø³Ø¨ Ø§Ù„Ù…ÙˆØ¶Ø¹
        if position == 0:
            insights.append(f"Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£ÙˆÙ„ '{letter}' ÙŠØ­Ø¯Ø¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©")
        elif position == len(word) - 1:
            insights.append(f"Ø§Ù„Ø­Ø±Ù Ø§Ù„Ø£Ø®ÙŠØ± '{letter}' ÙŠØ®ØªØªÙ… Ø§Ù„Ù…Ø¹Ù†Ù‰ ÙˆÙŠØ«Ø¨ØªÙ‡")
        else:
            insights.append(f"Ø§Ù„Ø­Ø±Ù '{letter}' ÙÙŠ Ø§Ù„Ù…ÙˆØ¶Ø¹ {position + 1} ÙŠØ±Ø¨Ø· Ø¨ÙŠÙ† Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¹Ù†Ù‰")

        # Ø±Ø¤Ù‰ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¹Ù†Ù‰
        meaning = self.letter_meanings.get(letter, '')
        if 'Ø§Ù„ÙˆØ­Ø¯Ø©' in meaning:
            insights.append("ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„ØªÙØ±Ø¯")
        elif 'Ø§Ù„Ø­ÙŠØ§Ø©' in meaning:
            insights.append("ÙŠØ¶ÙÙŠ Ù…Ø¹Ù†Ù‰ Ø§Ù„Ø­ÙŠÙˆÙŠØ© ÙˆØ§Ù„Ù†Ø´Ø§Ø·")
        elif 'Ø§Ù„Ø¹Ù„Ù…' in meaning:
            insights.append("ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø¥Ø¯Ø±Ø§Ùƒ")

        return insights

    def _extract_root(self, word: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©."""

        # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø±
        # ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§ Ù„ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        prefixes = ['Ø§Ù„', 'Ùˆ', 'Ù', 'Ø¨', 'Ùƒ', 'Ù„']
        suffixes = ['Ø©', 'Ø§Ù†', 'ÙŠÙ†', 'ÙˆÙ†', 'Ù‡Ø§', 'Ù‡Ù…', 'Ù‡Ù†', 'ÙƒÙ…', 'ÙƒÙ†']

        clean_word = word

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¨Ø§Ø¯Ø¦Ø§Øª
        for prefix in prefixes:
            if clean_word.startswith(prefix):
                clean_word = clean_word[len(prefix):]
                break

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù„ÙˆØ§Ø­Ù‚
        for suffix in suffixes:
            if clean_word.endswith(suffix):
                clean_word = clean_word[:-len(suffix)]
                break

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ø°Ø± Ø§Ù„Ø«Ù„Ø§Ø«ÙŠ (Ø£ÙˆÙ„ 3 Ø­Ø±ÙˆÙ Ø¹Ø§Ø¯Ø©)
        if len(clean_word) >= 3:
            return clean_word[:3]
        else:
            return clean_word

    def _apply_baserah_analysis(self, word: str, letter_analyses: List[LetterAnalysis]) -> Dict[str, float]:
        """ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ Baserah Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø©."""

        # Ø¬Ù…Ø¹ Ù‚ÙŠÙ… Ø§Ù„Ø­Ø±ÙˆÙ
        letter_values = [analysis.baserah_value for analysis in letter_analyses]

        if not letter_values:
            return {'total_value': 0.0, 'average_value': 0.0, 'harmony_index': 0.0}

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        total_value = sum(letter_values)
        average_value = total_value / len(letter_values)

        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„ØªÙ†Ø§ØºÙ… (Ù…Ø¯Ù‰ ØªÙ‚Ø§Ø±Ø¨ Ù‚ÙŠÙ… Ø§Ù„Ø­Ø±ÙˆÙ)
        variance = sum((v - average_value) ** 2 for v in letter_values) / len(letter_values)
        harmony_index = baserah_sigmoid(1.0 - variance, n=1, k=3.0, x0=0.5, alpha=1.0)

        # ØªØ·Ø¨ÙŠÙ‚ Ø¯ÙˆØ§Ù„ Baserah Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        quantum_value = baserah_quantum_sigmoid(average_value, n=1000, k=2.0, x0=0.5, alpha=1.2)
        linear_trend = baserah_linear(average_value, slope=1.5, intercept=0.1)

        return {
            'total_value': total_value,
            'average_value': average_value,
            'harmony_index': harmony_index,
            'quantum_value': quantum_value,
            'linear_trend': linear_trend,
            'baserah_signature': f"{total_value:.3f}_{harmony_index:.3f}_{quantum_value:.3f}"
        }

    def _apply_basil_theories_to_word(self, word: str, letter_analyses: List[LetterAnalysis]) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø©."""

        basil_theories = {}

        # Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±
        positive_letters = [l for l in letter_analyses if l.position % 2 == 0]
        negative_letters = [l for l in letter_analyses if l.position % 2 == 1]

        positive_sum = sum(l.baserah_value for l in positive_letters)
        negative_sum = sum(l.baserah_value for l in negative_letters)
        balance = abs(positive_sum - negative_sum)

        basil_theories['zero_duality'] = {
            'positive_sum': positive_sum,
            'negative_sum': negative_sum,
            'balance': balance,
            'is_balanced': balance < 0.1,
            'theory': 'Zero Duality Theory - Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ±'
        }

        # Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯
        if len(letter_analyses) >= 2:
            first_half = letter_analyses[:len(letter_analyses)//2]
            second_half = letter_analyses[len(letter_analyses)//2:]

            first_half_avg = sum(l.baserah_value for l in first_half) / len(first_half)
            second_half_avg = sum(l.baserah_value for l in second_half) / len(second_half)

            perpendicular_angle = abs(first_half_avg - second_half_avg) * 90

            basil_theories['perpendicular_opposites'] = {
                'first_half_value': first_half_avg,
                'second_half_value': second_half_avg,
                'perpendicular_angle': perpendicular_angle,
                'is_perpendicular': abs(perpendicular_angle - 90) < 10,
                'theory': 'Perpendicular Opposites Theory - ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯'
            }

        # Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„
        filaments = []
        for i, letter_analysis in enumerate(letter_analyses):
            filament = {
                'filament_id': i,
                'letter': letter_analysis.letter,
                'value': letter_analysis.baserah_value,
                'meaning_contribution': letter_analysis.semantic_contribution,
                'is_primary': letter_analysis.baserah_value > 0.5
            }
            filaments.append(filament)

        basil_theories['filament_theory'] = {
            'filaments': filaments,
            'total_filaments': len(filaments),
            'primary_filaments': len([f for f in filaments if f['is_primary']]),
            'reconstruction_possible': len(filaments) > 0,
            'theory': 'Filament Theory - Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„'
        }

        return basil_theories

    def _calculate_semantic_weight(self, word: str, letter_analyses: List[LetterAnalysis],
                                 baserah_analysis: Dict[str, float]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ Ù„Ù„ÙƒÙ„Ù…Ø©."""

        # Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        length_factor = baserah_sigmoid(len(word) / 10.0, n=1, k=1.0, x0=0.5, alpha=1.0)
        harmony_factor = baserah_analysis.get('harmony_index', 0.5)
        contribution_factor = sum(l.semantic_contribution for l in letter_analyses) / len(letter_analyses) if letter_analyses else 0.0

        # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        semantic_weight = (length_factor + harmony_factor + contribution_factor) / 3.0

        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ
        return baserah_sigmoid(semantic_weight * 2.0, n=1, k=2.0, x0=0.5, alpha=1.0)

    def _find_related_words(self, word: str, root: str) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©."""

        related_words = []

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù† ÙƒÙ„Ù…Ø§Øª Ø¨Ù†ÙØ³ Ø§Ù„Ø¬Ø°Ø±
        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            cursor.execute('SELECT word FROM lexicon_entries WHERE root = ? AND word != ?', (root, word))
            results = cursor.fetchall()

            related_words = [result[0] for result in results]

            conn.close()

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©: {e}")

        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯
        if not related_words and root:
            # ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ø¨Ù†ÙØ³ Ø§Ù„Ø¬Ø°Ø± (ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§)
            common_patterns = [f"{root}Ø©", f"Ù…{root}", f"{root}ÙŠ", f"{root}Ø§Ù†"]
            related_words = [pattern for pattern in common_patterns if pattern != word]

        return related_words[:5]  # Ø£ÙˆÙ„ 5 ÙƒÙ„Ù…Ø§Øª

    def _save_lexicon_entry(self, word: str, data: Dict[str, Any], source: LexiconSource):
        """Ø­ÙØ¸ Ù…Ø¯Ø®Ù„ Ù…Ø¹Ø¬Ù…ÙŠ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""

        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            cursor.execute('''
                INSERT OR REPLACE INTO lexicon_entries
                (word, root, meaning, detailed_meaning, source, letter_analysis, usage_examples, related_words, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                word,
                data.get('root', ''),
                data.get('meaning', ''),
                data.get('detailed_meaning', ''),
                source.value,
                json.dumps(data.get('letter_analysis', {}), ensure_ascii=False),
                json.dumps(data.get('usage_examples', []), ensure_ascii=False),
                json.dumps(data.get('related_words', []), ensure_ascii=False),
                json.dumps({'source': source.value}, ensure_ascii=False)
            ))

            conn.commit()
            conn.close()

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ: {e}")

    def _save_analysis_to_database(self, lexicon_entry: LexiconEntry, letter_analyses: List[LetterAnalysis]):
        """Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª."""

        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            # Ø­ÙØ¸ Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ
            cursor.execute('''
                INSERT OR REPLACE INTO lexicon_entries
                (word, root, meaning, detailed_meaning, source, letter_analysis, baserah_analysis,
                 basil_theories, semantic_weight, usage_examples, related_words, metadata)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                lexicon_entry.word,
                lexicon_entry.root,
                lexicon_entry.meaning,
                lexicon_entry.detailed_meaning,
                lexicon_entry.source.value,
                json.dumps(lexicon_entry.letter_analysis, ensure_ascii=False),
                json.dumps(lexicon_entry.baserah_analysis, ensure_ascii=False),
                json.dumps(lexicon_entry.basil_theories_application, ensure_ascii=False),
                lexicon_entry.semantic_weight,
                json.dumps(lexicon_entry.usage_examples, ensure_ascii=False),
                json.dumps(lexicon_entry.related_words, ensure_ascii=False),
                json.dumps(lexicon_entry.metadata, ensure_ascii=False)
            ))

            # Ø­ÙØ¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
            for letter_analysis in letter_analyses:
                cursor.execute('''
                    INSERT INTO letter_analyses
                    (word, letter, position, meaning, baserah_value, basil_theory,
                     semantic_contribution, revolutionary_insights)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    lexicon_entry.word,
                    letter_analysis.letter,
                    letter_analysis.position,
                    letter_analysis.meaning,
                    letter_analysis.baserah_value,
                    letter_analysis.basil_theory_applied,
                    letter_analysis.semantic_contribution,
                    json.dumps(letter_analysis.revolutionary_insights, ensure_ascii=False)
                ))

            conn.commit()
            conn.close()

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")

    def _update_engine_statistics(self, lexicon_entry: LexiconEntry, letter_analyses: List[LetterAnalysis]):
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø­Ø±Ùƒ."""

        self.engine_stats['words_analyzed'] += 1
        self.engine_stats['letters_analyzed'] += len(letter_analyses)
        self.engine_stats['meanings_extracted'] += 1
        self.engine_stats['baserah_analyses_performed'] += 1

        if lexicon_entry.root:
            self.engine_stats['roots_discovered'] += 1

        if lexicon_entry.basil_theories_application:
            self.engine_stats['basil_theories_applications'] += len(lexicon_entry.basil_theories_application)

        # ØªØ­Ø¯ÙŠØ« Ù…ØªÙˆØ³Ø· Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        current_avg = self.engine_stats['average_semantic_weight']
        words_count = self.engine_stats['words_analyzed']
        new_avg = ((current_avg * (words_count - 1)) + lexicon_entry.semantic_weight) / words_count
        self.engine_stats['average_semantic_weight'] = new_avg

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø«ÙˆØ±ÙŠØ©
        total_insights = sum(len(l.revolutionary_insights) for l in letter_analyses)
        self.engine_stats['total_revolutionary_insights'] += total_insights

    def search_word_meanings(self, word: str, include_related: bool = True) -> Dict[str, Any]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ø¹ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ."""

        print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø¹Ø§Ù†ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø©: {word}")

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø©
        lexicon_entry = self.analyze_word_revolutionary(word, deep_analysis=True)

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        search_results = {
            'word': word,
            'root': lexicon_entry.root,
            'main_meaning': lexicon_entry.meaning,
            'detailed_meaning': lexicon_entry.detailed_meaning,
            'semantic_weight': lexicon_entry.semantic_weight,
            'letter_analysis': lexicon_entry.letter_analysis,
            'baserah_analysis': lexicon_entry.baserah_analysis,
            'basil_theories': lexicon_entry.basil_theories_application,
            'revolutionary_insights': [],
            'related_words': lexicon_entry.related_words if include_related else [],
            'usage_examples': lexicon_entry.usage_examples
        }

        # Ø¬Ù…Ø¹ Ø§Ù„Ø±Ø¤Ù‰ Ø§Ù„Ø«ÙˆØ±ÙŠØ© Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
        letter_analyses = self._analyze_letters_revolutionary(word)
        for letter_analysis in letter_analyses:
            search_results['revolutionary_insights'].extend(letter_analysis.revolutionary_insights)

        return search_results

    def explore_letter_meanings(self, letter: str) -> Dict[str, Any]:
        """Ø§Ø³ØªÙƒØ´Ø§Ù Ø¯Ù„Ø§Ù„Ø§Øª Ø­Ø±Ù Ù…Ø¹ÙŠÙ†."""

        if letter not in self.letter_meanings:
            return {
                'letter': letter,
                'meaning': 'Ø­Ø±Ù ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ',
                'error': 'Ø§Ù„Ø­Ø±Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ'
            }

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±Ù
        letter_value = ord(letter) / 1000.0
        baserah_value = baserah_sigmoid(letter_value, n=1, k=1.5, x0=0.5, alpha=1.0)
        quantum_value = baserah_quantum_sigmoid(letter_value, n=1000, k=2.0, x0=0.5, alpha=1.2)

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø­Ø±Ù
        words_with_letter = self._find_words_with_letter(letter)

        return {
            'letter': letter,
            'meaning': self.letter_meanings[letter],
            'baserah_value': baserah_value,
            'quantum_value': quantum_value,
            'frequency_in_database': len(words_with_letter),
            'example_words': words_with_letter[:10],  # Ø£ÙˆÙ„ 10 ÙƒÙ„Ù…Ø§Øª
            'revolutionary_analysis': {
                'semantic_power': baserah_value,
                'quantum_signature': quantum_value,
                'letter_position_in_alphabet': ord(letter) - ord('Ø§') + 1 if 'Ø§' <= letter <= 'ÙŠ' else -1
            }
        }

    def _find_words_with_letter(self, letter: str) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙƒÙ„Ù…Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø­Ø±Ù Ù…Ø¹ÙŠÙ†."""

        words = []

        try:
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            cursor.execute('SELECT word FROM lexicon_entries WHERE word LIKE ?', (f'%{letter}%',))
            results = cursor.fetchall()

            words = [result[0] for result in results]

            conn.close()

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {e}")

        # Ø¥Ø¶Ø§ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ù…Ø¯Ù…Ø¬
        for word in self.integrated_lexicons[LexiconSource.CUSTOM_DATABASE]:
            if letter in word and word not in words:
                words.append(word)

        return words

    def analyze_text_revolutionary(self, text: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø«ÙˆØ±ÙŠ Ø´Ø§Ù…Ù„ Ù„Ù†Øµ ÙƒØ§Ù…Ù„."""

        print(f"ğŸ“ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ Ù„Ù„Ù†Øµ...")

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        words = text.split()

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ ÙƒÙ„Ù…Ø©
        word_analyses = []
        total_semantic_weight = 0.0
        all_letters = []
        all_roots = set()

        for word in words:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø© Ù…Ù† Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØªØ±Ù‚ÙŠÙ…
            clean_word = ''.join(c for c in word if c.isalpha())

            if clean_word:
                try:
                    analysis = self.analyze_word_revolutionary(clean_word, deep_analysis=False)
                    word_analyses.append(analysis)
                    total_semantic_weight += analysis.semantic_weight
                    all_letters.extend(list(clean_word))
                    if analysis.root:
                        all_roots.add(analysis.root)
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒÙ„Ù…Ø© '{clean_word}': {e}")

        # ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠ Ù„Ù„Ù†Øµ
        text_statistics = {
            'total_words': len(words),
            'analyzed_words': len(word_analyses),
            'total_letters': len(all_letters),
            'unique_letters': len(set(all_letters)),
            'unique_roots': len(all_roots),
            'average_semantic_weight': total_semantic_weight / len(word_analyses) if word_analyses else 0.0
        }

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙƒØ±Ø§Ø±Ø§Ù‹
        letter_frequency = {}
        for letter in all_letters:
            letter_frequency[letter] = letter_frequency.get(letter, 0) + 1

        most_frequent_letters = sorted(letter_frequency.items(), key=lambda x: x[1], reverse=True)[:5]

        # ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ø§Ù‹
        text_basil_analysis = self._apply_basil_theories_to_text(word_analyses)

        return {
            'original_text': text,
            'text_statistics': text_statistics,
            'word_analyses': [
                {
                    'word': analysis.word,
                    'meaning': analysis.meaning,
                    'semantic_weight': analysis.semantic_weight,
                    'root': analysis.root
                } for analysis in word_analyses
            ],
            'letter_frequency': dict(most_frequent_letters),
            'most_frequent_letters_meanings': [
                {
                    'letter': letter,
                    'frequency': freq,
                    'meaning': self.letter_meanings.get(letter, 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
                } for letter, freq in most_frequent_letters
            ],
            'unique_roots': list(all_roots),
            'basil_theories_analysis': text_basil_analysis,
            'revolutionary_insights': self._generate_text_insights(word_analyses, text_statistics),
            'analysis_metadata': {
                'analysis_time': datetime.now().isoformat(),
                'engine_version': self.version,
                'revolutionary_method': True
            }
        }

    def _apply_basil_theories_to_text(self, word_analyses: List[LexiconEntry]) -> Dict[str, Any]:
        """ØªØ·Ø¨ÙŠÙ‚ Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ ÙƒØ§Ù…Ù„Ø§Ù‹."""

        if not word_analyses:
            return {}

        # Ø¬Ù…Ø¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØ©
        semantic_weights = [analysis.semantic_weight for analysis in word_analyses]

        # Ù†Ø¸Ø±ÙŠØ© Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„ØµÙØ± Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Øµ
        positive_words = [w for i, w in enumerate(word_analyses) if i % 2 == 0]
        negative_words = [w for i, w in enumerate(word_analyses) if i % 2 == 1]

        positive_sum = sum(w.semantic_weight for w in positive_words)
        negative_sum = sum(w.semantic_weight for w in negative_words)
        text_balance = abs(positive_sum - negative_sum)

        # Ù†Ø¸Ø±ÙŠØ© ØªØ¹Ø§Ù…Ø¯ Ø§Ù„Ø£Ø¶Ø¯Ø§Ø¯
        first_half = word_analyses[:len(word_analyses)//2]
        second_half = word_analyses[len(word_analyses)//2:]

        first_half_avg = sum(w.semantic_weight for w in first_half) / len(first_half) if first_half else 0
        second_half_avg = sum(w.semantic_weight for w in second_half) / len(second_half) if second_half else 0

        # Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙØªØ§Ø¦Ù„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù†Øµ
        text_filaments = []
        for i, word_analysis in enumerate(word_analyses):
            filament = {
                'word': word_analysis.word,
                'semantic_weight': word_analysis.semantic_weight,
                'position': i,
                'is_primary': word_analysis.semantic_weight > 0.5
            }
            text_filaments.append(filament)

        return {
            'zero_duality_text': {
                'positive_sum': positive_sum,
                'negative_sum': negative_sum,
                'balance': text_balance,
                'is_balanced': text_balance < 0.1,
                'balance_percentage': (1 - text_balance) * 100
            },
            'perpendicular_opposites_text': {
                'first_half_avg': first_half_avg,
                'second_half_avg': second_half_avg,
                'difference': abs(first_half_avg - second_half_avg),
                'is_perpendicular': abs(first_half_avg - second_half_avg) > 0.3
            },
            'filament_theory_text': {
                'total_filaments': len(text_filaments),
                'primary_filaments': len([f for f in text_filaments if f['is_primary']]),
                'filament_density': len(text_filaments) / len(word_analyses) if word_analyses else 0,
                'strongest_filament': max(text_filaments, key=lambda x: x['semantic_weight']) if text_filaments else None
            }
        }

    def _generate_text_insights(self, word_analyses: List[LexiconEntry],
                              text_statistics: Dict[str, Any]) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø«ÙˆØ±ÙŠØ© Ù„Ù„Ù†Øµ."""

        insights = []

        # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„Ø·ÙˆÙ„ ÙˆØ§Ù„ØªØ¹Ù‚ÙŠØ¯
        if text_statistics['total_words'] > 20:
            insights.append("Ø§Ù„Ù†Øµ Ø·ÙˆÙŠÙ„ ÙˆÙ…Ø¹Ù‚Ø¯ØŒ ÙŠØ­ØªØ§Ø¬ ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø¹Ø§Ù†ÙŠ")
        elif text_statistics['total_words'] < 5:
            insights.append("Ø§Ù„Ù†Øµ Ù‚ØµÙŠØ± ÙˆÙ…Ø±ÙƒØ²ØŒ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ Ù…ÙƒØ«ÙØ©")

        # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„Ù„ØºÙˆÙŠ
        diversity_ratio = text_statistics['unique_roots'] / text_statistics['analyzed_words'] if text_statistics['analyzed_words'] > 0 else 0
        if diversity_ratio > 0.7:
            insights.append("Ø§Ù„Ù†Øµ ÙŠØªÙ…ÙŠØ² Ø¨ØªÙ†ÙˆØ¹ Ù„ØºÙˆÙŠ Ø¹Ø§Ù„ÙŠ ÙˆØ«Ø±Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¬Ø°ÙˆØ±")
        elif diversity_ratio < 0.3:
            insights.append("Ø§Ù„Ù†Øµ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­Ø¯ÙˆØ¯Ø© Ù…Ù† Ø§Ù„Ø¬Ø°ÙˆØ± ÙˆØ§Ù„Ù…Ø¹Ø§Ù†ÙŠ")

        # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ
        avg_weight = text_statistics['average_semantic_weight']
        if avg_weight > 0.7:
            insights.append("Ø§Ù„Ù†Øµ ÙŠØ­Ù…Ù„ ÙˆØ²Ù†Ø§Ù‹ Ø¯Ù„Ø§Ù„ÙŠØ§Ù‹ Ø¹Ø§Ù„ÙŠØ§Ù‹ ÙˆÙ…Ø¹Ø§Ù†ÙŠ Ù‚ÙˆÙŠØ©")
        elif avg_weight < 0.3:
            insights.append("Ø§Ù„Ù†Øµ Ø®ÙÙŠÙ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠØŒ Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ ØªØ¹Ù…ÙŠÙ‚ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù†ÙŠ")

        # Ø±Ø¤Ù‰ Ø­ÙˆÙ„ Ø§Ù„Ø­Ø±ÙˆÙ
        if text_statistics['unique_letters'] > 20:
            insights.append("Ø§Ù„Ù†Øµ ÙŠØ³ØªØ®Ø¯Ù… Ù…Ø¹Ø¸Ù… Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ù…Ù…Ø§ ÙŠØ¯Ù„ Ø¹Ù„Ù‰ Ø«Ø±Ø§Ø¡ Ù„ØºÙˆÙŠ")

        return insights

    def get_engine_status(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù…."""

        return {
            'engine_info': {
                'name': self.engine_name,
                'version': self.version,
                'creation_time': self.creation_time.isoformat(),
                'type': 'arabic_lexicon_engine'
            },
            'statistics': self.engine_stats,
            'capabilities': {
                'word_analysis': True,
                'letter_analysis': True,
                'text_analysis': True,
                'root_extraction': True,
                'baserah_integration': True,
                'basil_theories_application': True,
                'revolutionary_insights': True,
                'database_storage': True
            },
            'integrated_lexicons': {
                source.value: len(data) for source, data in self.integrated_lexicons.items()
            },
            'letter_meanings_count': len(self.letter_meanings),
            'database_path': self.lexicon_db_path,
            'revolutionary_features': {
                'baserah_pure_approach': True,
                'basil_theories_integration': True,
                'letter_semantic_analysis': True,
                'quantum_linguistic_processing': True,
                'revolutionary_insights_generation': True
            }
        }

    def export_analysis_results(self, output_file: str, format_type: str = 'json') -> bool:
        """ØªØµØ¯ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„."""

        try:
            # Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØµØ¯ÙŠØ±
            export_data = {
                'engine_info': {
                    'name': self.engine_name,
                    'version': self.version,
                    'export_time': datetime.now().isoformat()
                },
                'statistics': self.engine_stats,
                'letter_meanings': self.letter_meanings,
                'analysis_results': []
            }

            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            conn = sqlite3.connect(self.lexicon_db_path)
            cursor = conn.cursor()

            cursor.execute('SELECT * FROM lexicon_entries ORDER BY created_at DESC LIMIT 100')
            results = cursor.fetchall()

            for result in results:
                analysis_result = {
                    'word': result[1],
                    'root': result[2],
                    'meaning': result[3],
                    'semantic_weight': result[9],
                    'created_at': result[13]
                }
                export_data['analysis_results'].append(analysis_result)

            conn.close()

            # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if format_type.lower() == 'json':
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(export_data, f, ensure_ascii=False, indent=2)
            else:
                return False

            print(f"âœ… ØªÙ… ØªØµØ¯ÙŠØ± Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰: {output_file}")
            return True

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØµØ¯ÙŠØ±: {e}")
            return False


# === Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø±ÙŠØ¹ ===

def create_arabic_lexicon_engine(engine_name: str = "QuickArabicLexicon") -> ArabicLexiconEngine:
    """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ù…Ø¹Ø¬Ù… Ø¹Ø±Ø¨ÙŠ Ø³Ø±ÙŠØ¹."""

    engine = ArabicLexiconEngine(engine_name)
    print(f"ğŸ“š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {engine_name}")
    return engine


def analyze_arabic_word(word: str, engine_name: str = "QuickAnalyzer") -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„ÙƒÙ„Ù…Ø© Ø¹Ø±Ø¨ÙŠØ©."""

    engine = ArabicLexiconEngine(engine_name)
    result = engine.search_word_meanings(word, include_related=True)
    return result


def explore_arabic_letter(letter: str, engine_name: str = "QuickExplorer") -> Dict[str, Any]:
    """Ø§Ø³ØªÙƒØ´Ø§Ù Ø³Ø±ÙŠØ¹ Ù„Ø­Ø±Ù Ø¹Ø±Ø¨ÙŠ."""

    engine = ArabicLexiconEngine(engine_name)
    result = engine.explore_letter_meanings(letter)
    return result


def analyze_arabic_text(text: str, engine_name: str = "QuickTextAnalyzer") -> Dict[str, Any]:
    """ØªØ­Ù„ÙŠÙ„ Ø³Ø±ÙŠØ¹ Ù„Ù†Øµ Ø¹Ø±Ø¨ÙŠ."""

    engine = ArabicLexiconEngine(engine_name)
    result = engine.analyze_text_revolutionary(text)
    return result


# === Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ===

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ."""

    print("ğŸ“šâœ¨ Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ ÙÙŠ Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø«ÙˆØ±ÙŠ!")
    print("ğŸ”¤ Ù…Ø¯Ø¹ÙˆÙ… Ø¨Ù†Ø¸Ø±ÙŠØ§Øª Ø¨Ø§Ø³Ù„ ÙˆÙ†Ø¸Ø§Ù… Baserah Ù„Ø§Ø³ØªÙƒØ´Ø§Ù Ø¯Ù„Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±ÙˆÙ")
    print()

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ø±Ùƒ
    engine = ArabicLexiconEngine("MainArabicLexiconEngine")

    # Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
    print("ğŸ” Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«ÙˆØ±ÙŠ:")

    # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø©
    word_analysis = engine.search_word_meanings("Ø§Ù„Ù„Ù‡")
    print(f"\nğŸ“ ØªØ­Ù„ÙŠÙ„ ÙƒÙ„Ù…Ø© 'Ø§Ù„Ù„Ù‡':")
    print(f"   Ø§Ù„Ù…Ø¹Ù†Ù‰: {word_analysis['main_meaning']}")
    print(f"   Ø§Ù„Ø¬Ø°Ø±: {word_analysis['root']}")
    print(f"   Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ: {word_analysis['semantic_weight']:.3f}")

    # ØªØ­Ù„ÙŠÙ„ Ø­Ø±Ù
    letter_analysis = engine.explore_letter_meanings("Ø¹")
    print(f"\nğŸ”¤ ØªØ­Ù„ÙŠÙ„ Ø­Ø±Ù 'Ø¹':")
    print(f"   Ø§Ù„Ù…Ø¹Ù†Ù‰: {letter_analysis['meaning']}")
    print(f"   Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø«ÙˆØ±ÙŠØ©: {letter_analysis['baserah_value']:.3f}")

    # Ø¹Ø±Ø¶ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ
    status = engine.get_engine_status()
    print(f"\nğŸ“Š Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ:")
    print(f"   ÙƒÙ„Ù…Ø§Øª Ù…Ø­Ù„Ù„Ø©: {status['statistics']['words_analyzed']}")
    print(f"   Ø­Ø±ÙˆÙ Ù…Ø­Ù„Ù„Ø©: {status['statistics']['letters_analyzed']}")
    print(f"   Ø±Ø¤Ù‰ Ø«ÙˆØ±ÙŠØ©: {status['statistics']['total_revolutionary_insights']}")


if __name__ == "__main__":
    main()
